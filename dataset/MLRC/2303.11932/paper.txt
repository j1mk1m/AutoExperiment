<figure id="fig:teaser">
<figure>
<embed src="results/VOC/figures/qualitative/teaserv2_0.pdf" />
</figure>
<figure>
<embed src="results/figures/waterbird_teaser.pdf" />
</figure>
<figcaption><strong>(a) Model guidance increases object focus.</strong>
Models may rely on irrelevant background features or spurious
correlations (presence of person provides positive evidence for bicycle,
center row, col. 1). Guiding the model via bounding box annotations can
mitigate this and consistently increases the focus on object features
(bottom row). <strong>(b) Model guidance can improve accuracy.</strong>
In the presence of spurious correlations in the training data,
non-guided models might focus on the wrong features. In the example
image in (b), the waterbird is incorrectly classified to be a landbird
due to the background (col. 3). Guiding the model via bounding box
annotation (as shown in col. 2), the model can be guided to focus on the
bird features for classification (col. 4). </figcaption>
</figure>

# Introduction {#sec:introduction}

::: figure*
![image](results/VOC/figures/qualitative/double_column_0.pdf){width=".95\\textwidth"}
:::

Deep neural networks (DNNs) excel at learning predictive features that
allow them to correctly classify a set of training images with ease. The
features learnt on the training set, however, do not necessarily
transfer to unseen images: , instead of learning the actual
class-relevant features, DNNs might memorize individual images (. ) or
exploit spurious correlations in the training data (cf. ). For example,
if bikes are highly correlated with people in the training data, a model
might learn to associate the presence of a person in an image as
positive evidence for a bike ([1](#fig:teaser){reference-type="ref"
reference="fig:teaser"}a, col. 1, rows 1-2), which can limit how well it
generalizes. Similarly, a bird classifier might rely on background
features from the bird's habitat, and fail to correctly classify in a
different habitat (cf. [1](#fig:teaser){reference-type="ref"
reference="fig:teaser"}b cols. 1-3 and ).

To detect such behaviour, recent advances in model interpretability have
provided attribution methods () to understand a model's reasoning. These
methods typically provide attention maps that highlight regions of
importance in an input to explain the model's decisions and can help
identify incorrect reasoning such as reliance on spurious or irrelevant
features, see for example [1](#fig:teaser){reference-type="ref"
reference="fig:teaser"}b.

As many attribution methods are in fact themselves differentiable (),
recent work has explored the idea of using them to guide the models to
make them "right for the right reasons" . Specifically, models can be
guided by jointly optimizing for correct classification as well as for
attributing importance to regions deemed relevant by humans. This can
help the model focus on the relevant features of a class, and correct
errors in reasoning ([1](#fig:teaser){reference-type="ref"
reference="fig:teaser"}b, col. 4). Such guidance has the added benefit
of providing well-localized explanations that are thus easier to
understand for end users
([\[fig:teaser2\]](#fig:teaser2){reference-type="ref"
reference="fig:teaser2"}).

While model guidance has shown promising results, a detailed study of
how to do this most *effectively* is crucially missing. In particular,
model guidance has so far been studied for a limited set of attribution
methods and models and usually on relatively simple and/or synthetic
datasets; further, the evaluation settings between approaches can
significantly differ, which makes a fair comparison difficult.

Therefore, in this work, we perform an in-depth evaluation of model
guidance on large scale, real-world datasets, to better understand the
effectiveness of a variety of design choices. Specifically, we evaluate
model guidance along the following dimensions: the model architecture,
the guidance *depth*[^1], the attribution method, and the loss function.
In this context, we propose using the EPG score [@wang2020score]---an
evaluation metric that has thus far been used to evaluate the quality of
attribution methods---as an additional loss function (which we call the
loss) as it is fully differentiable.

Further, as annotation costs can be a major hurdle for making model
guidance practical, we place a particular focus on *efficient* guidance.
Specifically, we use bounding boxes instead of semantic segmentation
masks, and evaluate the robustness of guidance techniques under limited
or overly coarse annotations to reduce data collection costs.

We find that our loss lends itself well to those settings. On the one
hand, it exhibits a high degree of robustness to limited or noisy
bounding box annotations
(cf. [\[fig:coarse_annotations,fig:lim_local_input\]](#fig:coarse_annotations,fig:lim_local_input){reference-type="ref"
reference="fig:coarse_annotations,fig:lim_local_input"}). On the other
hand, despite the coarseness of bounding box guidance, it maintains a
clear focus on object-specific features inside the bounding boxes, see
[1](#fig:teaser){reference-type="ref" reference="fig:teaser"}a, row 3.
In contrast, prior approaches often regularize for a uniform
distribution of the attribution values inside the annotation masks, and
thus tend to exhibit much lower attribution granularity
(cf. [7](#fig:loss_comp){reference-type="ref"
reference="fig:loss_comp"}).

**(1)** We perform an in-depth evaluation of model guidance on
challenging large scale, multi-label classification datasets (, ),
assessing the impact of attribution methods, model architectures,
guidance depths, and loss functions. Further, we show that, despite
being relatively coarse, bounding box supervision can provide sufficient
guidance to the models whilst being much cheaper to obtain than semantic
segmentation masks. **(2)** We propose using the Energy Pointing Game
(EPG) score as an alternative to the IoU metric for evaluating the
effectiveness of such guidance and show that the EPG score constitutes a
good loss function for model guidance, particularly when using bounding
boxes. **(3)** We show that model guidance can be performed
cost-effectively by using annotation masks that are noisy or are
available for only a small fraction ($1\%$) of the training data.
**(4)** We show through experiments on the dataset that model guidance
with a small number of annotations suffices to improve the model's
generalization under distribution shifts at test time.

# Related Work {#sec:related}

are often used to explain black-box models by generating heatmaps that
highlight input regions important to the model's decision. However, such
methods are often not faithful to the model and risk misleading users.
Recent work proposes inherently interpretable models that address this
by providing model-faithful explanations by design. In our work, we use
both popular post-hoc and model-inherent attribution methods to guide
models and discuss their effectiveness.

Several approaches have been proposed for training better models by
enforcing desirable properties on their attributions. These include
enforcing consistency against augmentations , smoothness , separation of
classes , or constraining the model's attention . In contrast, in this
work, we focus on providing explicit human guidance to the model using
bounding box annotations. This constitutes more explicit guidance but
allows fine-grained control over the model's reasoning even with few
annotations.

In contrast to the indirect regularization effect achieved by
attribution priors, various approaches have been proposed (cf. ) to
actively guide models by regularizing their attributions, for tasks such
as classification , segmentation , VQA , and knowledge distillation .
The goal of such approaches is not only to improve performance, but also
make sure that the model is "right for the right reasons" . For
classifiers, this typically involves jointly optimizing both for
classification performance and localization to object features. While
various benefits of model guidance have been reported, most prior work
evaluate on simple datasets and, thus far, no common evaluation setting
has emerged. Recently, has extended model guidance to ImageNet, showing
that its benefits can scale to large scale problems. In contrast to ,
who investigated one particular attribution method , our focus lies on a
better understanding of the impact of the different design choices for
model guidance.

To distill the most effective techniques for model guidance, in this
work, we conduct an in-depth evaluation on challenging,commonly used
real-world multi-label classification datasets (, ). Specifically, we
perform a comprehensive comparison across multiple dimensions of
interest: the loss function, the model architecture, the guidance depth,
and the attribution method. For this, we evaluate the localization
losses introduced in the closest related work, , , and ; additionally,
we propose using the EPG metric [@wang2020score] as a loss function and
show that it has various desirable properties, in particular when
guiding models via bounding box annotations.

Finally, model guidance has also been used to mitigate reliance on
spurious features using language guidance , and we show that using a
small number of coarse bounding box annotations can be similarly
effective.

The benefits of model guidance have typically been shown via
improvements in classification performance () or an increase in between
object masks and attribution maps (). In addition to these metrics, we
also evaluate on the metric , which has thus far only been used to
evaluate the quality of the attribution methods themselves. We further
show that it lends itself well to being used as a guidance loss, as it
places only minor constraints on the model, and, in contrast to the IoU
metric, it is fully differentiable.

# Guiding Models Using Attributions {#sec:method}

![**Model guidance overview.** We jointly optimize for classification
($\mathcal L_\text{class}$) and localization of attributions to
human-annotated bounding boxes ($\mathcal L_\text{loc}$), to guide the
model to focus on object features. Various localization loss functions
can be used, see [3.4](#sec:method:losses){reference-type="ref"
reference="sec:method:losses"}.
](results/ModelGuidingPipeline.png){#fig:pipeline width="\\linewidth"}

In this section, we provide an overview of the model guidance approach
that jointly optimizes for classification and localization
([3.1](#sec:method:procedure){reference-type="ref"
reference="sec:method:procedure"}). Specifically, we describe the
attribution methods
([3.2](#sec:method:attributions){reference-type="ref"
reference="sec:method:attributions"}), metrics
([3.3](#sec:method:metrics){reference-type="ref"
reference="sec:method:metrics"}), and localization loss functions
([3.4](#sec:method:losses){reference-type="ref"
reference="sec:method:losses"}) that we evaluate in
[5](#sec:results){reference-type="ref" reference="sec:results"}. In
[3.5](#sec:method:efficient){reference-type="ref"
reference="sec:method:efficient"} we discuss our strategy to train for
localization in the presence of multiple ground truth classes.

We consider a multi-label classification problem with $K$ classes with
$X\myin\mathbb{R}^{C\times H\times W}$ the input image and
$y\myin\{0,1\}^K$ the one-hot encoding of the image labels. With
$A_k\myin\mathbb{R}^{H\times W}$ we denote an attribution map for a
class $k$ for $X$ using a classifier $f$; $A_{k}^+$ denotes the positive
component of the attributions,
$\hat{A}_k=\frac{A_k}{\max(\text{abs}(A_k))}$ normalized attributions,
and $\hat{A}_{k}^+=\frac{A_{k}^+}{\max(A_{k}^+)}$ normalized positive
attributions. Finally, $M_k \myin \{0,1\}^{H\times W}$ denotes the
binary mask for class $k$, which is given by the union of bounding boxes
of all occurrences of class $k$ in $X$.

## Model Guidance Procedure {#sec:method:procedure}

Following prior work (), the model is trained jointly for classification
and localization (cf. [2](#fig:pipeline){reference-type="ref"
reference="fig:pipeline"}): $$\label{eq:overall}
\textstyle
    \mathcal{L}=\mathcal{L}_\text{class} + \lambda_\text{loc}\mathcal{L}_\text{loc}\;.$$
, the loss consists of a classification loss
($\mathcal{L}_\text{class}$), for which we use binary cross-entropy, and
a localization loss ($\mathcal{L}_\text{loc}$), which we discuss in
[3.4](#sec:method:losses){reference-type="ref"
reference="sec:method:losses"}; here, the hyperparameter
$\lambda_\text{loc}$ controls the weight given to each of the
objectives.

## Attribution Methods {#sec:method:attributions}

In contrast to prior work that typically use attributions, we perform an
evaluation over a selection of popularly used differentiable[^2]
attribution methods which have been shown to localize well : , , and .
We further evaluate model-inherent explanations of the recently proposed
models . To ensure comparability across attribution methods , we
evaluate all attribution methods at the input, various intermediate, and
the final spatial layer.

IxG computes the element-wise product $\odot$ of the input and the gradients
of the $k$-th output w.r.t. the input, $X\odot\nabla_X f_k(X)$. For
piece-wise linear models such as DNNs with ReLU activations , this
faithfully computes the linear contributions of a given input pixel to
the model output.

GradCam computes importance attributions as a ReLU-thresholded,
gradient-weighted sum of activation maps. In detail, it is given by
$\text{ReLU}(\sum_c \alpha_c^k \odot U_c)$ with $c$ denoting the channel
dimension, and $\alpha^k$ the average-pooled gradients of the output for
class $k$ with respect to the activations $U$ of the last convolutional
layer in the model.

IntGrad takes an axiomatic approach and is formulated as the integral of
gradients over a straight line path from a baseline input to the given
input $X$. Approximating this integral requires several gradient
computations, making it computationally expensive for use in model
guidance. To alleviate this, when optimizing with , we use the recently
proposed models that allow for an exact computation of in a single
backward pass.

B-cos attributions are generated using the inherently-interpretable networks,
which promote alignment between the input $\mathbf x$ and a dynamic
weight matrix $\mathbf W(\mathbf x)$ during optimization. In our
experiments, we use the contribution maps given by the element-wise
product of the dynamic weights with the input
($\mathbf W^T_k(\mathbf x)\odot \mathbf x$), which faithfully represent
the contribution of each pixel to class $k$. To be able to guide models,
we developed a differentiable implementation of explanations, see
supplement.

## Evaluation Metrics {#sec:method:metrics}

We evaluate the models' performance on both our training objectives:
classification and localization. For classification, we use the F1 score
and mean average precision (). We discuss the localization metrics
below.

is a commonly used metric (cf. ) that computes the intersection between
the ground truth annotation masks and the binarized attribution maps,
normalized by their union; for binarization, a threshold parameter needs
to be chosen. In this work, the ground truth masks are taken to be the
union of all bounding boxes of a class in the image and, following prior
work , the threshold parameter is selected via a heldout set.

measures the concentration of attribution energy within the mask, the
fraction of positive attributions inside the bounding boxes:
$$\label{eq:epg}
    \text{EPG}_k = \frac{\sum_{h=1}^H\sum_{w=1}^W M_{k,hw} A^+_{k,hw}}{\sum_{h=1}^H\sum_{w=1}^W A^+_{k,hw}}\;.$$
In contrast to , more faithfully takes into account the relative
importance given to each input region, since it does not binarize the
attributions. Like , the scores lie in $[0,1]$, with higher scores
indicating better localization.

## Localization Losses {#sec:method:losses}

We evaluate the most commonly used localization losses
($\mathcal{L}_\text{loc}$ in
[\[eq:overall\]](#eq:overall){reference-type="ref"
reference="eq:overall"}) from prior work. We describe these losses as
applied on attribution maps of an image for a single class $k$, as well
as the proposed EPG-derived loss.

minimizes the $L_1$ distance between annotation masks and normalized
positive attributions $\hat A_{k}^+$, guiding the model towards uniform
attributions inside the mask and suppressing attributions outside of it.
$$\label{eq:lone}
    \textstyle
    \mathcal{L}_{\text{loc},k} = \frac{1}{H\times W}\sum_{h=1}^H\sum_{w=1}^W\lVert M_{k,hw} - \hat{A}_{k,hw}^+ \rVert_1$$

Per-pixel cross entropy loss (PPCE) applies a binary cross entropy loss between the mask and the normalized
positive annotations $\hat A_{k}^+$, thus guiding the model to maximize
the attributions inside the mask: $$\label{eq:ppce}
\textstyle
    \mathcal{L}_{\text{loc},k} = -\frac{1}{\lVert M_k \rVert_1}\sum_{h=1}^H\sum_{w=1}^W M_{k,hw}\log(\hat{A}_{k,hw}^+) \;.$$
As PPCE does not constrain attributions outside the mask, there is no
explicit pressure to avoid spurious features.

RRR* introduced the RRR loss to regularize the normalized input gradients
$\hat{A}_{k,hw}$ as $$\label{eq:rrr}
    \textstyle \mathcal{L}_{\text{loc},k} = \sum_{h=1}^H\sum_{w=1}^W (1-M_{k,hw}) \hat{A}_{k,hw}^2 \;.$$
To extend it to our setting, we take $\hat{A}_{k,hw}$ to be given by an
arbitrary attribution method (); we denote this generalized version by RRR*.
In contrast to the loss, only regularizes attributions *outside* the
ground truth masks. While it thus does not introduce a uniformity prior
similar to the loss, it also does not explicitly promote high importance
attributions inside the masks.

[]{#sec:method:energyloss label="sec:method:energyloss"} In addition to
the losses described in prior work, we propose to also evaluate using
the score (, [\[eq:epg\]](#eq:epg){reference-type="ref"
reference="eq:epg"}) as a loss function for model guidance, as it is
fully differentiable. In particular, we simply define it as
$$\label{eq:energyloss}
\textstyle
    \mathcal{L}_{\text{loc},k} = -\text{EPG}_k.$$ Unlike existing
localization losses that either (i) do not constrain attributions across
the entire input (, ), or (ii) force the model to attribute uniformly
within the mask even if it includes irrelevant background regions (, ),
maximizing the score jointly optimizes for higher attribution energy
within the mask and lower attribution energy outside the mask. By not
enforcing a uniformity prior, we find that the loss is able to provide
effective guidance while allowing the model to learn freely what to
focus on within the bounding boxes
([5](#sec:results){reference-type="ref" reference="sec:results"}).

## Efficient Optimization {#sec:method:efficient}

In contrast to prior work , we perform model guidance on a multi-label
classification setting, and consequently there are multiple ground truth
classes whose attribution localization could be optimized. Computing and
optimizing for several attributions within an image would add a
significant overhead to the computational cost of training (multiple
backward passes). Hence, for efficiency, we sample one ground truth
class $k$ per image at random for every batch and only optimize for
localization of that class, ,
$\mathcal{L}_\text{loc}\myeq\mathcal{L}_{\text{loc},k}$. We find that
this still provides effective model guidance while keeping the training
cost tractable.

# Experimental Setup {#sec:experiments}

![**Selecting models for evaluation.** For each configuration, we
evaluate every model at every checkpoint and measure its performance
across various metrics (, , ) on the validation set; every point in the
left graph corresponds to one model (for models optimized via the loss
at the input layer). Instead of evaluating a single model on the test
set, we evaluate *all Pareto-dominant* models, as indicated in the
center and right plot.
](results/VOC/figures/pareto_example.pdf){#fig:pareto_example
width="1.025\\columnwidth"}

In this section, we describe our experimental setup and how we select
the best models across metrics; for full details, see supplement. We
evaluate across all possible choices for each category, and discuss our
results in [5](#sec:results){reference-type="ref"
reference="sec:results"}.

We evaluate on and for multi-label image classification. In
[5.5](#sec:results:waterbirds){reference-type="ref"
reference="sec:results:waterbirds"}, to understand the effectiveness of
model guidance in mitigating spurious correlations, we also evaluate on
the synthetically constructed Waterbirds-100 dataset , where landbirds
are perfectly correlated with land backgrounds on the training and
validation sets, but are equally likely to occur on land or water in the
test set (similar for waterbirds and water). With this dataset, we
evaluate model guidance for suppressing undesired features.

As described in [3.2](#sec:method:attributions){reference-type="ref"
reference="sec:method:attributions"}, we evaluate with , , , and using
models with a backbone. For , we use an to reduce the computational
cost, and a for the attributions. To emphasize that the results
generalize across different backbones, we further provide results for a
ViT-S and a DenseNet-121 . We evaluate optimizing the attributions at
different network layers, such as at the input image and the last
convolutional layers' output[^3], as well as at multiple intermediate
layers. Within the main paper, we highlight some of the most
representative and insightful results, the full set of results can be
found in the supplement. All models were pretrained on , and model
guidance was applied when fine-tuning the models on the target dataset.

As described in [3.4](#sec:method:losses){reference-type="ref"
reference="sec:method:losses"}, we compare four localization losses in
our evaluation: (i) , (ii) , (iii) , and (iv)
(cf. [3.4](#sec:method:losses){reference-type="ref"
reference="sec:method:losses"}, ).

As discussed in [3.3](#sec:method:metrics){reference-type="ref"
reference="sec:method:metrics"}, we evaluate both for classification and
localization performance of the models. For classification, we report
the F1 scores, similar results with scores can be found in the
supplement. For localization, we evaluate using the and scores.

As we evaluate for two distinct objectives (classification +
localization), it is not trivial to decide which models perform 'the
best', a model that provides the best classification performance might
provide significantly worse localization than a model that provides only
slightly lower classification performance. Finding the right balance and
deciding which of those models in fact constitutes the 'better' model
depends on the preference of the end user. Hence, instead of selecting
models based on a single metric, we select the set of Pareto-dominant
models across three metrics---F1, , and ---for each training
configuration, as defined by a combination of attribution method, layer,
and loss. Specifically, as shown in
[3](#fig:pareto_example){reference-type="ref"
reference="fig:pareto_example"}, we train each configuration using three
different choices of $\lambda_\text{loc}$, and select the set of
Pareto-dominant models among all checkpoints (epochs and
$\lambda_\text{loc}$). This provides a more holistic view of the general
trends on the effectiveness of model guidance for each configuration.

# Experimental Results {#sec:results}

::: figure*
![**PASCAL VOC results for vs. .**
](results/VOC/figures/loc/all_results_f1.pdf){#fig:epg_voc
width="\\textwidth"}

![**MS COCO results for vs. .**
](results/COCO/figures/loc/all_results_f1.pdf){#fig:epg_coco
width="\\textwidth"}
:::

::: figure*
<figure>
<p><span><embed
src="results/VOC/figures/iou/final/all_results.pdf" /></span></p>
</figure>
:::

![**vs. on VOC.** We observe the same trends as in
[4](#fig:epg_voc){reference-type="ref" reference="fig:epg_voc"} for
different backbone architectures, specifically a B-cos DenseNet-121 and
a B-cos ViT-S. For results, see
supplement.](results/VOC/figures/backbones/architectures_loc.pdf){#fig:moremodels
width="\\linewidth"}

::: figure*
![image](results/VOC/figures/loc/tdes_results.pdf){width="\\linewidth"}
:::

In this section, we discuss our experimental findings. In particular, in
[5.1](#sec:results:epg+iou){reference-type="ref"
reference="sec:results:epg+iou"}, we first discuss the impact of the
loss functions on the and scores of the models; in
[5.2](#sec:results:layers+models){reference-type="ref"
reference="sec:results:layers+models"}, we then analyze the impact of
the models and attribution methods; further in
[5.3](#sec:results:accuracy){reference-type="ref"
reference="sec:results:accuracy"}, we show that guiding the models via
their explanations can lead to improved classification accuracy. In
[5.4](#sec:results:ablations){reference-type="ref"
reference="sec:results:ablations"}, we present additional studies in
which we evaluate and discuss the cost of model guidance approaches: in
particular, we study model guidance with limited additional labels, with
increasingly coarse bounding boxes, and at deep layers in the network.
Finally, in [5.5](#sec:results:waterbirds){reference-type="ref"
reference="sec:results:waterbirds"}, we show the utility of model
guidance in improving accuracy in the presence of distribution shifts.
For easier reference, we label our individual findings as --.

To draw conclusive insights and highlight general and reliable trends in
the experiments, we compare the Pareto curves (see
[3](#fig:pareto_example){reference-type="ref"
reference="fig:pareto_example"}) of individual configurations. If the
Pareto curve of a specific loss (in
[\[fig:epg_results\]](#fig:epg_results){reference-type="ref"
reference="fig:epg_results"}) consistently Pareto-dominates the Pareto
curves of all other losses, we can confidently conclude that for the
combination of evaluated metrics (vs. ), this loss is the best choice.

## Comparing loss functions for model guidance {#sec:results:epg+iou}

In the following, we highlight the main insights gained from the
*quantitative* evaluations. For a *qualitative* comparison between the
losses, please see [7](#fig:loss_comp){reference-type="ref"
reference="fig:loss_comp"}; note that we show examples for a model as
the differences become clearest; full results can be found in the
supplement.

In [\[fig:epg_results\]](#fig:epg_results){reference-type="ref"
reference="fig:epg_results"}, we plot the Pareto curves for vs. scores
for a wide range of configurations (see
[4](#sec:experiments){reference-type="ref" reference="sec:experiments"})
on (a) and (b); specifically, we group the results by model type (, , ),
the layer depths at which the attribution was regularized (Input /
Final), and the loss used during optimization (, , , ). From these
results it becomes apparent that the optimization with the loss yields
the best trade-off between accuracy () and the score: , when looking at
the upper right plot in [4](#fig:epg_voc){reference-type="ref"
reference="fig:epg_voc"} we can see that the loss (red dots) improves
over the baseline model (white cross) by improving the localization in
terms of score with only a minor cost in classification performance
(score). Further trading off F1 scores yields even higher scores.
Importantly, the loss Pareto-dominates all the other losses (: blue
diamonds; : green triangles; : yellow pentagons). This is is also true
for the other network types (, [4](#fig:epg_voc){reference-type="ref"
reference="fig:epg_voc"} (top left), and ,
[4](#fig:epg_voc){reference-type="ref" reference="fig:epg_voc"} (top
center)) and at the final layer (bottom row), and generalizes across
backbone architectures ([6](#fig:moremodels){reference-type="ref"
reference="fig:moremodels"}). When comparing
[4](#fig:epg_voc){reference-type="ref" reference="fig:epg_voc"} and
[5](#fig:epg_coco){reference-type="ref" reference="fig:epg_coco"}, we
also find these results to be highly consistent between datasets.

Similarly, in
[\[fig:iou_results\]](#fig:iou_results){reference-type="ref"
reference="fig:iou_results"}, we plot the Pareto curves of vs. scores
for various configurations at the final layer; for the results at the
input layer and on the dataset, please see the supplement. For , the
loss provides the best trade-off and, with few exceptions, -guided
models Pareto-dominate all other models in all configurations.

By not forcing the models to highlight the entire bounding boxes (see
[\[sec:method:energyloss\]](#sec:method:energyloss){reference-type="ref"
reference="sec:method:energyloss"}), we find that the loss also
suppresses background features *within* the bounding boxes, thus better
preserving fine details of the explanations
(cf. [\[fig:loss_comp,,fig:dilation_comp\]](#fig:loss_comp,,fig:dilation_comp){reference-type="ref"
reference="fig:loss_comp,,fig:dilation_comp"}). To quantify this, we
evaluate the distribution of ([\[eq:epg\]](#eq:epg){reference-type="ref"
reference="eq:epg"}) just within the bounding boxes. For this, we take
advantage of the segmentation mask annotations available for a subset of
the test set. Specifically, we measure the contained in the segmentation
masks versus the entire bounding box, which indicates how much of the
attributions actually highlight on-object features. We find that the
loss outperforms across all models and configurations; see supplement
for details.

![**Loss comparison** for input attributions (atts.) of a model. We show
atts. before (baseline, col. 2) and after guidance (cols. 3-6) for a
specific image (col. 1) and its bounding box annotation. We find that
and yield sparse atts, whereas yields smoother atts, as it is optimized
to fill the entire bounding box. For we observe only a minor effect on
the atts.
](results/VOC/figures/qualitative/loss_comp.pdf){#fig:loss_comp
width="\\columnwidth"}

In short, we find that the loss works best for improving the metric,
whereas the loss yields the highest gains in terms of ; depending on the
use case, either of these losses could thus be recommendable. However,
we find that the loss is more robust to annotation errors (,
[5.4](#sec:results:ablations){reference-type="ref"
reference="sec:results:ablations"}), and, as discussed in , the loss
more reliably focuses on object-specific features.

## Comparing models and attribution methods {#sec:results:layers+models}

In the following, we highlight our findings regarding different
attribution methods and models. Given the similarity of the results
between and , and since attributions performed better than for models,
we show results in the supplement.

We find that the models not only achieve the highest /performance before
applying model guidance, ('baselines') but also obtain the highest gains
in and and thus the highest overall performance (for see
[\[fig:epg_results\]](#fig:epg_results){reference-type="ref"
reference="fig:epg_results"}, right; for , see supplement): , an -based
model achieves an score of 71.7 @ 79.4% , thus significantly
outperforming the best scores of both other model types at a much lower
cost in (: 55.8 @ 69.0%, : 62.3 @ 68.9%). This is also observed
*qualitatively*, as we show in the supplement.

As can be seen in
[\[fig:epg_results\]](#fig:epg_results){reference-type="ref"
reference="fig:epg_results"} (bottom) and
[\[fig:iou_results\]](#fig:iou_results){reference-type="ref"
reference="fig:iou_results"}, all models can be guided well via
regularization at the final layer, all models show improvements in and
score.

In short, we find model guidance to work well across all tested models
when optimizing at the final layer (), highlighting its wide
applicability. However, to obtain highly detailed and well-localized
attributions at the input layer, the model-inherent explanations of the
models seem to lend themselves much better to such guidance ().

## Improving accuracy with model guidance {#sec:results:accuracy}

For both the models (final layer) and the (input+final), we found models
that improve the localization metrics *and* the score. These
improvements are particularly pronounced for the : , we find models that
improve the and scores by $\Delta\myeq7.2$ p.p. and $\Delta\myeq1.4$
p.p. respectively
([\[fig:epg_results\]](#fig:epg_results){reference-type="ref"
reference="fig:epg_results"}, center top), or the and scores by
$\Delta\myeq11.9$ p.p. and $\Delta\myeq1.4$
p.p. ([\[fig:iou_results\]](#fig:iou_results){reference-type="ref"
reference="fig:iou_results"}, center).

However, overall we observe a trade-off between localization and
accuracy
([\[fig:epg_results,fig:iou_results\]](#fig:epg_results,fig:iou_results){reference-type="ref"
reference="fig:epg_results,fig:iou_results"}). Given the similarity of
the training and test distributions, focusing on the object need not
improve classification performance, as spurious features are also
present at test time. Further, the guided model is discouraged from
relying on contextual features, making the classification more
challenging. In [5.5](#sec:results:waterbirds){reference-type="ref"
reference="sec:results:waterbirds"}, we show that guidance can
significantly improve performance when there is a distribution shift
between training and test.

## Efficiency and robustness considerations {#sec:results:ablations}

While bounding boxes decrease the data collection cost with respect to
segmentation masks, they can nonetheless be expensive to obtain,
especially when expert knowledge is required. To further reduce those
costs, in this section, we assess the robustness of guiding the model
with a limited number () or increasingly coarse annotations (). Apart
from *data efficiency*, we further explore how *training efficiency* can
be improved for fine-grained (input-level) explanations (), as
explanations at early layers are more costly to obtain than those at
later layers.

In [10](#fig:lim_local_input){reference-type="ref"
reference="fig:lim_local_input"}, we show that the score can be
significantly improved with a very limited number of annotations; for
results, see supplement. Specifically, we find that when using only 1%
of the training data (25 annotated images) for , improvements of up to
$\Delta\myeq23.0$ p.p. ($\Delta\myeq1.4$) in () can be obtained, at a
minor drop in ($\Delta\myeq0.3$ p.p. and $\Delta\myeq2.5$
p.p. respectively). When annotating up to 10% of the images, very
similar results can be achieved as with full annotation (see cols. 2+3
in [10](#fig:lim_local_input){reference-type="ref"
reference="fig:lim_local_input"}).

As discussed in
[\[sec:method:energyloss\]](#sec:method:energyloss){reference-type="ref"
reference="sec:method:energyloss"}, the loss only directs the model on
which features *not* to use and does not impose a uniform prior on the
attributions within the bounding boxes. As a result, we find it to be
much more stable to annotation errors: , in
[8](#fig:coarse_annotations){reference-type="ref"
reference="fig:coarse_annotations"}, we visualize how the (top) and
(bottom) scores of the best performing models under the (left) and loss
(right) evolve when using coarser bounding boxes; for this, we simply
dilate the bounding box size by $p\myin\{10, 25, 50\}$% during training,
see [9](#fig:dilation_comp){reference-type="ref"
reference="fig:dilation_comp"}. While the models optimized via the loss
achieve increasingly worse results (right), the -optimized models are
essentially unaffected by the coarseness of the annotations.

![**Quantitative results for dilated bounding boxes** for a model at the
input layer. We show and (top and bottom) results for models trained
with various amounts of annotation errors (increasingly large bounding
boxes, see [9](#fig:dilation_comp){reference-type="ref"
reference="fig:dilation_comp"}). The loss yields highly consistent
results despite training with heavily dilated bounding boxes (left),
whereas the results of the loss (right) worsen markedly; best viewed in
color.
](results/VOC/figures/input/coarse_bbox_results_bcos.pdf){#fig:coarse_annotations
width="1.025\\columnwidth"}

![**Qualitative results for dilated bounding boxes** for a model at
input. Examples for attributions (rows 2+3) of models trained with
dilated bounding boxes (row 1). In contrast to , models trained with
show significant gains in object focus even with significant noise
('Baseline' vs. '50%').
](results/VOC/figures/qualitative/dilation_comp.pdf){#fig:dilation_comp
width="\\columnwidth"}

In short, we find that the models can be guided effectively at a low
cost in terms of annotation effort, as only few annotations ( for ) are
required (cf. ), and, especially for the loss, these annotations can be
very coarse and do not have to be 'pixel-perfect' (cf. ).

While guided input-level explanations of networks exhibit a high degree
of detail, regularizing those explanations comes at an added training
cost. In particular, optimizing at the input layer requires
backpropagating through the entire network to compute the attributions.
In an effort to reduce training costs whilst maintaining the benefits of
fine-grained explanations at input resolution, we evaluate if
input-level attributions benefit from an optimization at deeper layers.

Specifically, we regularize attributions at the final and at three
intermediate layers (Mid{1,2,3}), and evaluate the localization of
attributions at the input. We find
([\[fig:speedup\]](#fig:speedup){reference-type="ref"
reference="fig:speedup"}) that training at a deeper layer can provide
significant speed-ups in training time with often a negligible cost in
localization performance. , since we do not have to compute a full
backward pass through the entire model during training, optimizing at
Mid2 (col. 2 in [\[fig:speedup\]](#fig:speedup){reference-type="ref"
reference="fig:speedup"}) provides similar gains in localization but
with a 1.7x speed-up in training time.

![**results with limited annotations** for a model at the input layer,
optimized with the and the loss. Using bounding box annotations for as
little as 1% (left) of the images yields significant improvements in ,
and with 10% (center) similar gains as in the fully annotated setting
(right) are obtained.
](results/VOC/figures/localisation/input/limited_annotation_results.pdf){#fig:lim_local_input
width="1.025\\columnwidth"}

## Effectiveness against spurious correlations {#sec:results:waterbirds}

To evaluate the potential for mitigating spurious correlations, we
evaluate model guidance with the and losses on the synthetically
constructed dataset . We perform model guidance under two settings: (1)
the conventional setting to classify between landbirds and waterbirds,
using the region within the bounding box as the mask; and (2) the
reversed setting [@petryk2022guiding] to classify the background, , land
vs. water, using the region outside the bounding box as the mask. To
simulate a limited annotation budget, we only use bounding boxes for a
random 1% of the training set, and report results averaged over four
runs. We show the results for the worst-group accuracy (, images
containing a waterbird on land) and the overall accuracy using models in
[1](#tab:waterbirds){reference-type="ref" reference="tab:waterbirds"};
full results for all attributions and models can be found in the
supplement.

Both losses consistently and significantly improve the accuracy in the
conventional and the reversed settings by guiding the model to select
the 'right' features, birds (conventional) or background (reversed).
This guidance can also be observed qualitatively
(cf. [11](#fig:waterbirds){reference-type="ref"
reference="fig:waterbirds"}).

![**Qualitative results.** Without guidance, a model might focus on the
background to classify birds (baseline) and thus misclassify waterbirds
on land (col. 2). Guided models can correct such errors and focus on the
desired feature: in cols. 3+4 (5+6) the model is guided to classify by
using the bird (background) features and arrives at the desired
prediction. Model predictions and confidence scores are indicated below
the images. ](results/figures/waterbird_example.pdf){#fig:waterbirds
width="\\linewidth"}

::: {#tab:waterbirds}
                            Conventional                                                                       Reversed                 
  ----------- ---------------------------------------- --------------------------------------- ---------------------------------------- ---------------------------------------
     Model                     Worst                                   Overall                                  Worst                                   Overall
   Baseline       43.4 ($\pm$`<!-- -->`{=html}2.4)        68.7 ($\pm$`<!-- -->`{=html}0.2)         56.6 ($\pm$`<!-- -->`{=html}2.4)        80.1 ($\pm$`<!-- -->`{=html}0.2)
    Energy      **56.1 ($\pm$`<!-- -->`{=html}4.0)**    **71.2 ($\pm$`<!-- -->`{=html}0.1)**     **62.8 ($\pm$`<!-- -->`{=html}2.1)**    **83.6 ($\pm$`<!-- -->`{=html}1.1)**
     $L_1$        51.1 ($\pm$`<!-- -->`{=html}1.9)        69.5 ($\pm$`<!-- -->`{=html}0.2)         58.8 ($\pm$`<!-- -->`{=html}5.0)        82.2 ($\pm$`<!-- -->`{=html}0.9)

  : **Waterbirds-100 results.** We find that model guidance is effective
  in improving both worst-group ('Waterbird on Land') and overall
  accuracy in the conventional (Landbird vs. Waterbird) and reversed
  (Land vs. Water) settings; full results in the supplement.
:::

# Discussion And Conclusion {#sec:conclusion}

In this work, we comprehensively evaluated various models, attribution
methods, and loss functions for their utility in guiding models to be
"right for the right reasons".

In summary, we find that guiding models via bounding boxes can
significantly improve and performance of the optimized attribution
method, with the loss working best to improve the score () and the loss
yielding the highest gains in scores (). While the models achieve the
best results in and score at the input layer (), all tested model types
(, , ) lend themselves well to being optimized at the final layer (),
which can even improve attribution maps at early layers (). Further, we
find that regularizing the explanations of the models and thereby
'telling them where to look' can increase the object recognition
performance (mAP/accuracy) of some models (), especially when strong
spurious correlations are present
([5.5](#sec:results:waterbirds){reference-type="ref"
reference="sec:results:waterbirds"}). Interestingly, those gains (, ),
can be achieved with relatively little additional annotation (). Lastly,
we find that by not assuming a uniform prior over the attributions
within the annotated bounding boxes, training with the energy loss is
more robust to annotation errors () and results in models that produce
attribution maps that are more focused on class-specific features ().

[^1]: The layer at which guidance is applied, typically at the last
    convolutional layer for [@selvaraju2017grad] or the first layer for
    [@shrikumar2017learning].

[^2]: Differentiability is necessary for optimizing attributions via
    gradient descent, so non-differentiable methods () are not
    considered.

[^3]: As typically used in (input) and (final) respectively.
