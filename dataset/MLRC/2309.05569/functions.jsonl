{"paper_id": "2309.05569", "func_id": "0", "file": "iti_gen/model.py", "name": "__init__", "header_line": "30", "line_start": "31", "line_end": "57", "relevant_paper": "", "description": "Initializes and sets up various components needed for processing and model execution.\n\n:param self.args: An object containing various attributes necessary for setting up and processing\n\n"}
{"paper_id": "2309.05569", "func_id": "1", "file": "iti_gen/model.py", "name": "ori_text_feature_extraction", "header_line": "129", "line_start": "130", "line_end": "139", "relevant_paper": "", "description": "Computes and modifies text query embeddings using a CLIP model.\n\nThis function performs the following steps:\n1. Extracts token embeddings from tokenized text queries using the CLIP model.\n2. Adds positional embeddings to the token embeddings.\n3. Transforms the combined embeddings through multiple transformer layers.\n4. Applies layer normalization and computes the final text features.\n\n:return: None. The results are stored in the class variables `self.text_queries_embedding` and `self.ori_text_feature`."}
{"paper_id": "2309.05569", "func_id": "2", "file": "iti_gen/model.py", "name": "construct_fair_text_features", "header_line": "194", "line_start": "195", "line_end": "210", "relevant_paper": "Inspired by [@lester2021power; @jia2022visual], we propose prompt tuning\nfor inclusive generation. Specifically, for a given category $a^{m}_{k}$\nwithin attribute $\\mathcal{A}_{m}$, we inject $q$ *learnable* tokens\n$\\bm{S}^{m}_{k} \\in \\mathbb{R}^{q \\times e}$ after the original $\\bm{T}$\nto construct a new prompt\n$\\bm{P}_{k}^{m} = [\\bm{T}; \\bm{S}^{m}_{k}] \\in \\mathbb{R}^{(p+q) \\times e}$.\nBy querying the model $G$ with $\\bm{P}_{k}^{m}$, we can generate images\nexhibiting the characteristics of the corresponding category\n$a^{m}_{k}$. To differentiate the new tokens $\\bm{S}^{m}_k$ from the\noriginal prompt $\\bm{T}$, we refer to them as *inclusive tokens*.\n\nWhen jointly considering $M$ attributes, we aggregate $M$ separate\ninclusive tokens\n$\\bm{S}^{1}_{o_1}, \\bm{S}^{2}_{o_2}, \\dots, \\bm{S}^{M}_{o_M}$ to\nrepresent a specific category combination\n$(a^1_{o_1}, a^2_{o_2}, \\dots, a^M_{o_M})$, , the concept of (\"woman\",\n\"dark skin\", $\\dots$, \"young\"). We thus expect to create a unique\n$\\bm{S}_{o_1 o_2 \\dots o_M}$, $$\\begin{aligned}\n \\label{eq:aggregation}\n    \\bm{S}_{o_1 o_2 \\dots o_M} = f (\\bm{S}^{1}_{o_1}, \\bm{S}^{2}_{o_2}, \\dots, \\bm{S}^{M}_{o_M})\n\\end{aligned}$$ that can be injected after $\\bm{T}$ to generate images\nfor this particular category combination. The aggregation function $f$\nin Equation\u00a0[\\[eq:aggregation\\]](#eq:aggregation){reference-type=\"ref\"\nreference=\"eq:aggregation\"} should be able to take various numbers of\nattributes while maintaining the permutation invariant property[^3] with\nrespect to attributes. Common options include element-wise average, sum,\nand max operations. Following\u00a0[@mikolov2013efficient], we adopt\nelement-wise sum to preserve the text semantics without losing\ninformation[^4]. Finally, we define the *inclusive prompt set* as\nfollows: $$\\begin{aligned}\n \\label{eq:set} \n    \\mathcal{P}_\\text{total} = \\{ &\\bm{P}_{o_1 o_2 \\dots o_M} = [{\\color{blue}{\\bm{T}}};\\sum_{m=1}^{M} {\\color{my_pink}{\\bm{S}^{m}_{o_m}}}] \\in \\mathbb{R}^{(p+q) \\times e} ~| \\nonumber \\\\\n    & 1 \\leq o_1 \\leq K_1, \\dots, 1 \\leq o_M \\leq K_M \\}.\n\\end{aligned}$$ By uniformly sampling the prompts from\n$\\mathcal{P}_\\text{total}$ as the conditions to generate images using\nthe generative model $G$, we achieve inclusiveness across all attributes\n(see Figure\u00a0[\\[fig:overview\\]](#fig:overview){reference-type=\"ref\"\nreference=\"fig:overview\"}). *More generally speaking, the distribution\nof the generated data is directly correlated to the distribution of the\nprompts, which can be easily controlled.*", "description": "Modifies the text queries embedding by adding FairToken embeddings and then processes it through a series of transformer layers.\n\n:param text_queries_embedding: A tensor representing the text queries embedding with shape (108, 77, 768).\n:param self.index: A list of lists where each sublist contains indices indicating where to add or modify embeddings in 'x'.\n:param self.args.token_length: The length of the token which determines how many positions to modify in the tensor.\n:param self.fairtoken_model: A list of models used to generate embeddings for FairTokens where each model corresponds to a specific index group.\n:param self.clip_layers_num: Number of transformer layers to pass the modified embedding through.\n:param self.clip_model: The model used to transform and normalize the final modified embedding.\n\n:return: A tensor of the same shape as the input embedding (108, 77, 768) that has been transformed by the FairToken additions and processed through the specified number of transformer layers."}
{"paper_id": "2309.05569", "func_id": "3", "file": "iti_gen/model.py", "name": "cos_loss", "header_line": "212", "line_start": "213", "line_end": "238", "relevant_paper": "", "description": "Computes a modified cosine similarity loss between image features and text features, applying a masking strategy based on attribute categories.\n\n:param image_features: A tensor containing the features of images. The shape is (n_img, feature_dim).\n:param text_features: A tensor containing the features of text. The shape is (n_text, feature_dim).\n:param each_index: A list of indices that specify groupings within the text features, corresponding to different categories (e.g., skin tone types).\n:param data_cls: A tensor containing the category labels for each image, which corresponds to the expected category of text for each image. The shape is (n_img,).\n:param self.args.device: The device (CPU or GPU) on which to perform the computations.\n\n:return: A scalar representing the computed cosine similarity loss, adjusted with the mask based on category matching."}
{"paper_id": "2309.05569", "func_id": "4", "file": "iti_gen/model.py", "name": "iti_gen_loss", "header_line": "240", "line_start": "241", "line_end": "287", "relevant_paper": "Specifically, we define the direction alignment loss\n$\\mathcal{L}_\\text{dir}$ to maximize the cosine similarity between the\nimage direction and the prompt direction as follows: $$\\begin{aligned}\n \\label{equ:4}\n    \\mathcal{L}_\\text{dir}^{m}(\\bm{S}^{m}_i, \\bm{S}^{m}_j) = 1 - \\bigl\\langle \\Delta_{\\bm{I}}^{m} (i, j), \\Delta_{\\bm{P}}^{m} (i, j) \\bigl\\rangle.\n\\end{aligned}$$ Here, the image direction $\\Delta_{\\bm{I}}$ is defined\nas the difference of the averaged image embeddings between two\ncategories of the attribute $\\mathcal{A}_m$. Let\n$\\mathfrak{X}_{k}^{m} = \\frac{1}{|\\mathcal{B}_k|}\\sum_{y_n^m = a_k^m} E_\\text{img} (\\vx_n^m)$\nbe the averaged image embedding for category $a_k^m$; $|\\mathcal{B}_k|$\nis the number of images from category $a_k^m$ in each mini-batch. We\ndenote the image direction as follows: $$\\begin{aligned}\n    \\Delta_{\\bm{I}}^{m} (i, j) = \\mathfrak{X}_{i}^{m} - \\mathfrak{X}_{j}^{m}.\n\\end{aligned}$$ Similarly, the prompt direction $\\Delta_{\\bm{P}}$ is\ndefined as the difference of the averaged prompt embeddings between two\ncategories. Let\n$\\mathfrak{P}_{k}^{m} = \\frac{1}{|\\mathcal{P}_k^m|} \\sum_{\\bm{P} \\in \\mathcal{P}^m_k} E_\\text{text}(\\bm{P})$\nbe the averaged prompt embedding for attribute $a_k^m$. Specifically,\n$\\mathcal{P}^{m}_{k} = \\{ \\bm{P} \\in \\mathcal{P}_\\text{total} ~|~ o_m = k \\}$\nis a collection of prompts containing all the category combinations for\nother attributes given the category $a_k^m$ for attribute\n$\\mathcal{A}_m$ (cf.\u00a0Equation\u00a0[\\[eq:set\\]](#eq:set){reference-type=\"ref\"\nreference=\"eq:set\"}). Finally, we denote the prompt direction as\nfollows: $$\\begin{aligned}\n    \\Delta^{m}_{\\bm{P}} (i, j) = \\mathfrak{P}_{i}^{m} -  \\mathfrak{P}_{j}^{m}.\n\\end{aligned}$$ By inducing the direction alignment, we aim to\nfacilitate the prompt learning of more meaningful and nuanced\ndifferences between images from different categories.\n\nWe observe that direction alignment loss alone may result in language\ndrift\u00a0[@lu2020countering; @lee2019countering; @ruiz2022dreambooth] ---\nthe prompts slowly lose syntactic and semantic properties of language as\nthey only focus on solving the alignment task. To resolve this issue, we\ndesign a semantic consistency objective to regularize the training by\nmaximizing the cosine similarity between the learning prompts and the\noriginal input prompt (see\nFigure\u00a0[2](#fig:approach){reference-type=\"ref\"\nreference=\"fig:approach\"}): $$\\begin{aligned}\n \\label{equ:7}\n    \\mathcal{L}_\\text{sem}^{m}(\\bm{S}^{m}_i, \\bm{S}^{m}_j) = \\text{max} \\Bigl(0, \\lambda - \\bigl \\langle E_\\text{text}(\\bm{P}), E_\\text{text}(\\bm{T}) \\bigl\\rangle \\Bigl)\n\\end{aligned}$$ where $\\bm{P} \\in \\mathcal{P}^m_i \\cup \\mathcal{P}^m_j$\nand $\\lambda$ is a hyperparameter (see an analysis in Section\u00a0). This\nloss is crucial for generating high-quality images that remain faithful\nto the input prompt.", "description": "Computes two ITI (Inter-sample Textual Inversion) generation losses: directional loss and contrastive loss.\n\n:param image_features: A tensor containing features of images, typically with size (N, 768), where N is \n                       the number of images.\n:param text_features: A tensor containing features of text descriptions, typically with size (M, 768), \n                      where M is the number of text descriptions.\n:param cate_num: An integer representing the number of categories for the attribute in consideration\n                 (e.g., skin tone, gender, etc.).\n:param each_index: A list or tensor of indices indicating which text features correspond to each image feature \n                   category.\n:param data_cls: A tensor that denotes the class of each image feature, used to filter image features by class.\n\n:return: A tuple containing:\n         - loss_direction: A tensor representing the computed directional loss for the given features.\n         - loss_con: A tensor representing the computed contrastive loss for the given features.\n\nThe function utilizes combinations of categories to compute differences in image and text features, normalizing \nthese differences and computing logits that contribute to the final losses averaged over all combinations and \nindices."}
{"paper_id": "2309.05569", "func_id": "5", "file": "iti_gen/model.py", "name": "prompt_prepend", "header_line": "289", "line_start": "290", "line_end": "296", "relevant_paper": "", "description": "Processes and transforms pre-tokenized text queries using a CLIP model, and constructs fair text features.\n\n:param prepended_prompt: A prompt text that has already been prepared for tokenization.\n:return: Returns the result of construct_fair_text_features, which is likely a transformed feature set of the text queries and related indices, in the shape (108, 77, 768)."}
{"paper_id": "2309.05569", "func_id": "6", "file": "iti_gen/model.py", "name": "train", "header_line": "317", "line_start": "318", "line_end": "385", "relevant_paper": "", "description": "Iterates through training data to update and train models, computes losses, and saves model checkpoints.\n\nThis function performs training iteration over multiple attributes by adjusting models' training and evaluation modes,\ncomputing various losses based on image and text features, and updating model parameters through backpropagation.\n\nArguments:\n:param ep: Current epoch number during training.\n:param epoch_saving_list: A list of epoch numbers at which model checkpoints should be saved.\n:param folder_path: Path to the directory where model checkpoints will be saved.\n\nReturn Value:\n- None"}
{"paper_id": "2309.05569", "func_id": "7", "file": "dataloader/image_dataset.py", "name": "__init__", "header_line": "20", "line_start": "21", "line_end": "34", "relevant_paper": "", "description": "Initializes a data processing pipeline with specified directory, label, and upper bound while preparing a data list and setting up image transformations.\n\n:param root_dir: The root directory where the images are stored.\n:param label: A label or category associated with the data.\n:param upper_bound: An upper bound value used in data filtering or processing.\n:modifies self.root_dir: Sets the instance variable `self.root_dir` to the given parameter `root_dir`.\n:modifies self.label: Sets the instance variable `self.label` to the given parameter `label`.\n:modifies self.upper_bound: Sets the instance variable `self.upper_bound` to the given parameter `upper_bound`.\n:return: None\n\nThe function iterates over the directories in self.root_dir, searching for image files with the extensions '.jpg' and '.png'. \nIt creates a list of paths for each directory and randomly selects a number of images based on the lesser of self.upper_bound or the number of images available in the directory. \nSelected images are appended to self.file_list along with their associated labels from self.label."}
{"paper_id": "2309.05569", "func_id": "9", "file": "dataloader/image_dataset.py", "name": "__getitem__", "header_line": "50", "line_start": "51", "line_end": "54", "relevant_paper": "", "description": "Opens an image from a file path, applies transformations, and returns the image along with its label.\n\n:param idx: The index of the file in the file_list to be processed. It is expected to be an integer.\n:return: A dictionary containing the transformed image under the key 'img' and the associated label under the key 'label'."}
