{"functions": [{"paper_id": "2309.05569", "func_id": "0", "file": "iti_gen/model.py", "name": "__init__", "header_line": "30", "line_start": "31", "line_end": "57", "relevant_paper": ""}, {"paper_id": "2309.05569", "func_id": "1", "file": "iti_gen/model.py", "name": "ori_text_feature_extraction", "header_line": "129", "line_start": "130", "line_end": "139", "relevant_paper": ""}, {"paper_id": "2309.05569", "func_id": "2", "file": "iti_gen/model.py", "name": "construct_fair_text_features", "header_line": "194", "line_start": "195", "line_end": "210", "relevant_paper": "Inspired by [@lester2021power; @jia2022visual], we propose prompt tuning\nfor inclusive generation. Specifically, for a given category $a^{m}_{k}$\nwithin attribute $\\mathcal{A}_{m}$, we inject $q$ *learnable* tokens\n$\\bm{S}^{m}_{k} \\in \\mathbb{R}^{q \\times e}$ after the original $\\bm{T}$\nto construct a new prompt\n$\\bm{P}_{k}^{m} = [\\bm{T}; \\bm{S}^{m}_{k}] \\in \\mathbb{R}^{(p+q) \\times e}$.\nBy querying the model $G$ with $\\bm{P}_{k}^{m}$, we can generate images\nexhibiting the characteristics of the corresponding category\n$a^{m}_{k}$. To differentiate the new tokens $\\bm{S}^{m}_k$ from the\noriginal prompt $\\bm{T}$, we refer to them as *inclusive tokens*.\n\nWhen jointly considering $M$ attributes, we aggregate $M$ separate\ninclusive tokens\n$\\bm{S}^{1}_{o_1}, \\bm{S}^{2}_{o_2}, \\dots, \\bm{S}^{M}_{o_M}$ to\nrepresent a specific category combination\n$(a^1_{o_1}, a^2_{o_2}, \\dots, a^M_{o_M})$, , the concept of (\"woman\",\n\"dark skin\", $\\dots$, \"young\"). We thus expect to create a unique\n$\\bm{S}_{o_1 o_2 \\dots o_M}$, $$\\begin{aligned}\n \\label{eq:aggregation}\n    \\bm{S}_{o_1 o_2 \\dots o_M} = f (\\bm{S}^{1}_{o_1}, \\bm{S}^{2}_{o_2}, \\dots, \\bm{S}^{M}_{o_M})\n\\end{aligned}$$ that can be injected after $\\bm{T}$ to generate images\nfor this particular category combination. The aggregation function $f$\nin Equation\u00a0[\\[eq:aggregation\\]](#eq:aggregation){reference-type=\"ref\"\nreference=\"eq:aggregation\"} should be able to take various numbers of\nattributes while maintaining the permutation invariant property[^3] with\nrespect to attributes. Common options include element-wise average, sum,\nand max operations. Following\u00a0[@mikolov2013efficient], we adopt\nelement-wise sum to preserve the text semantics without losing\ninformation[^4]. Finally, we define the *inclusive prompt set* as\nfollows: $$\\begin{aligned}\n \\label{eq:set} \n    \\mathcal{P}_\\text{total} = \\{ &\\bm{P}_{o_1 o_2 \\dots o_M} = [{\\color{blue}{\\bm{T}}};\\sum_{m=1}^{M} {\\color{my_pink}{\\bm{S}^{m}_{o_m}}}] \\in \\mathbb{R}^{(p+q) \\times e} ~| \\nonumber \\\\\n    & 1 \\leq o_1 \\leq K_1, \\dots, 1 \\leq o_M \\leq K_M \\}.\n\\end{aligned}$$ By uniformly sampling the prompts from\n$\\mathcal{P}_\\text{total}$ as the conditions to generate images using\nthe generative model $G$, we achieve inclusiveness across all attributes\n(see Figure\u00a0[\\[fig:overview\\]](#fig:overview){reference-type=\"ref\"\nreference=\"fig:overview\"}). *More generally speaking, the distribution\nof the generated data is directly correlated to the distribution of the\nprompts, which can be easily controlled.*"}, {"paper_id": "2309.05569", "func_id": "3", "file": "iti_gen/model.py", "name": "cos_loss", "header_line": "212", "line_start": "213", "line_end": "238", "relevant_paper": ""}, {"paper_id": "2309.05569", "func_id": "4", "file": "iti_gen/model.py", "name": "iti_gen_loss", "header_line": "240", "line_start": "241", "line_end": "287", "relevant_paper": "Specifically, we define the direction alignment loss\n$\\mathcal{L}_\\text{dir}$ to maximize the cosine similarity between the\nimage direction and the prompt direction as follows: $$\\begin{aligned}\n \\label{equ:4}\n    \\mathcal{L}_\\text{dir}^{m}(\\bm{S}^{m}_i, \\bm{S}^{m}_j) = 1 - \\bigl\\langle \\Delta_{\\bm{I}}^{m} (i, j), \\Delta_{\\bm{P}}^{m} (i, j) \\bigl\\rangle.\n\\end{aligned}$$ Here, the image direction $\\Delta_{\\bm{I}}$ is defined\nas the difference of the averaged image embeddings between two\ncategories of the attribute $\\mathcal{A}_m$. Let\n$\\mathfrak{X}_{k}^{m} = \\frac{1}{|\\mathcal{B}_k|}\\sum_{y_n^m = a_k^m} E_\\text{img} (\\vx_n^m)$\nbe the averaged image embedding for category $a_k^m$; $|\\mathcal{B}_k|$\nis the number of images from category $a_k^m$ in each mini-batch. We\ndenote the image direction as follows: $$\\begin{aligned}\n    \\Delta_{\\bm{I}}^{m} (i, j) = \\mathfrak{X}_{i}^{m} - \\mathfrak{X}_{j}^{m}.\n\\end{aligned}$$ Similarly, the prompt direction $\\Delta_{\\bm{P}}$ is\ndefined as the difference of the averaged prompt embeddings between two\ncategories. Let\n$\\mathfrak{P}_{k}^{m} = \\frac{1}{|\\mathcal{P}_k^m|} \\sum_{\\bm{P} \\in \\mathcal{P}^m_k} E_\\text{text}(\\bm{P})$\nbe the averaged prompt embedding for attribute $a_k^m$. Specifically,\n$\\mathcal{P}^{m}_{k} = \\{ \\bm{P} \\in \\mathcal{P}_\\text{total} ~|~ o_m = k \\}$\nis a collection of prompts containing all the category combinations for\nother attributes given the category $a_k^m$ for attribute\n$\\mathcal{A}_m$ (cf.\u00a0Equation\u00a0[\\[eq:set\\]](#eq:set){reference-type=\"ref\"\nreference=\"eq:set\"}). Finally, we denote the prompt direction as\nfollows: $$\\begin{aligned}\n    \\Delta^{m}_{\\bm{P}} (i, j) = \\mathfrak{P}_{i}^{m} -  \\mathfrak{P}_{j}^{m}.\n\\end{aligned}$$ By inducing the direction alignment, we aim to\nfacilitate the prompt learning of more meaningful and nuanced\ndifferences between images from different categories.\n\nWe observe that direction alignment loss alone may result in language\ndrift\u00a0[@lu2020countering; @lee2019countering; @ruiz2022dreambooth] ---\nthe prompts slowly lose syntactic and semantic properties of language as\nthey only focus on solving the alignment task. To resolve this issue, we\ndesign a semantic consistency objective to regularize the training by\nmaximizing the cosine similarity between the learning prompts and the\noriginal input prompt (see\nFigure\u00a0[2](#fig:approach){reference-type=\"ref\"\nreference=\"fig:approach\"}): $$\\begin{aligned}\n \\label{equ:7}\n    \\mathcal{L}_\\text{sem}^{m}(\\bm{S}^{m}_i, \\bm{S}^{m}_j) = \\text{max} \\Bigl(0, \\lambda - \\bigl \\langle E_\\text{text}(\\bm{P}), E_\\text{text}(\\bm{T}) \\bigl\\rangle \\Bigl)\n\\end{aligned}$$ where $\\bm{P} \\in \\mathcal{P}^m_i \\cup \\mathcal{P}^m_j$\nand $\\lambda$ is a hyperparameter (see an analysis in Section\u00a0). This\nloss is crucial for generating high-quality images that remain faithful\nto the input prompt."}, {"paper_id": "2309.05569", "func_id": "5", "file": "iti_gen/model.py", "name": "prompt_prepend", "header_line": "289", "line_start": "290", "line_end": "296", "relevant_paper": ""}, {"paper_id": "2309.05569", "func_id": "6", "file": "iti_gen/model.py", "name": "train", "header_line": "317", "line_start": "318", "line_end": "385", "relevant_paper": ""}, {"paper_id": "2309.05569", "func_id": "7", "file": "dataloader/image_dataset.py", "name": "__init__", "header_line": "20", "line_start": "21", "line_end": "34", "relevant_paper": ""}, {"paper_id": "2309.05569", "func_id": "8", "file": "dataloader/image_dataset.py", "name": "prepare_datalist", "header_line": "36", "line_start": "37", "line_end": "45", "relevant_paper": ""}, {"paper_id": "2309.05569", "func_id": "9", "file": "dataloader/image_dataset.py", "name": "__getitem__", "header_line": "50", "line_start": "51", "line_end": "54", "relevant_paper": ""}]}