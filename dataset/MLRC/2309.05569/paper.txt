# Introduction {#s_introduction}

![**(a)** Given a human-written prompt ("*a headshot of a person*"),
existing text-to-image models [@rombach2022high] can hardly synthesize
pictures representing minority groups (, people with eyeglasses in this
example). **(b)** Conventional hard prompt searching [@ding2021cogview]
is sub-optimal due to linguistic ambiguity. **(c)** We address these
problems by leveraging a small set of reference images for inclusive
text-to-image generation ().
](figure/fig_motivation.pdf){#fig: motivation width="1\\linewidth"}

In recent years we have witnessed a remarkable leap in text-based visual
content creation, driven by breakthroughs in generative
modeling [@sohl2015deep; @ho2020denoising; @ramesh2021zero; @ramesh2022hierarchical; @rombach2022high]
and the access to large-scale multimodal
datasets [@schuhmann2022laion; @karras2019style]. Particularly, publicly
released models, such as Stable Diffusion [@rombach2022high], have
matured to the point where they can produce highly realistic images
based on human-written prompts.

However, one major drawback of existing text-to-image models is that
they inherit biases from the training
data [@bianchi2023easily; @ramesh2022hierarchical; @rombach2022high; @cho2022dall; @bansal2022well]
and thus have yet to exhibit *inclusiveness* --- the generated images
based on the input text may reflect stereotypes, leading to the
exclusion of certain attributes or minority groups. For instance, given
the prompt "a headshot of a person",
Figure [1](#fig: motivation){reference-type="ref"
reference="fig: motivation"}(a) shows how a state-of-the-art system
generates about 92$\%$ images of subjects without eyeglasses, and only
8$\%$ with eyeglasses, showing a clear bias towards people without
eyeglasses. Alternatively, as shown in
Figure [1](#fig: motivation){reference-type="ref"
reference="fig: motivation"}(b), one could specify the attribute in the
prompt, resulting in better outcomes; however, this will still result in
a sub-optimal solution due to linguistic ambiguity. While inclusiveness
has been critical to responsible AI, existing text-to-image models are
still
lagging [@cho2022dall; @bansal2022well; @petsiuk2022human; @parraga2022debiasing; @luccioni2023stable].
In this work, we propose a new method that achieves inclusiveness[^1] in
text-to-image generation using only a few example images, as illustrated
in Figure [1](#fig: motivation){reference-type="ref"
reference="fig: motivation"}(c).

To advance inclusive generation, a straightforward way is to retrain or
fine-tune the model upon request, using *truly* inclusive training
data [@dhariwal2021diffusion; @zhao2020leveraging]. Doing so, however,
is insurmountably challenging as collecting large-scale training data
that is balanced/inclusive across all attributes of interest is
impractical, and training generative models is highly
compute-intensive [@schuhmann2022laion; @russakovsky2015imagenet; @dhariwal2021diffusion].
Another principled approach towards inclusiveness is to specify or
enumerate each category in natural language (, hard prompt
searching) [@ding2021cogview; @petsiuk2022human]. However, many
categories are difficult to specify with natural language (, skin tone)
or cannot be well synthesized by the existing models due to linguistic
ambiguity or model
misrepresentation [@hutchinson2022underspecification].

At first glance, these seem to paint a grim picture for inclusive
text-to-image generation. However, we argue that instead of specifying
attributes explicitly using descriptive natural language, images can
represent specific concepts or attributes more efficiently. Observing
the availability of a shared vision-language embedding in many
multimodal generative models [@radford2021learning], we raise the
question: *can we learn inclusive prompt embeddings using images as
guidance?*

To achieve this goal, we introduce , a novel and practical framework
that creates discriminative prompts based on readily available reference
images for **I**nclusive **T**ext-to-**I**mage **GEN**eration.
Concretely, we leverage the vision-language pre-trained CLIP
model [@radford2021learning] to obtain the embeddings of the reference
images and learnable prompts. In the joint embedding space, we design a
new training objective to align the directions of the image and prompt
features. The core idea is to translate the visual attribute differences
into natural language differences such that the generated images based
on the learned prompts can effectively represent all desired categories.
By equalizing the sampling process over the learned prompts, our method
guarantees inclusiveness for text-to-image generation.

We validate our framework with Stable Diffusion [@rombach2022high]. can
leverage reference images from different domains, including human
faces [@liu2015faceattributes; @karkkainen2019fairface; @Feng:TRUST:ECCV2022]
and scenes [@skorokhodov2021aligning], to achieve inclusive generation
in single or multiple attributes of interest. needs neither prompt
specification nor model fine-tuning, bypassing the problems of
linguistic ambiguity as well as computational complexity. Moreover, is
compatible with the existing text-based image generation models (,
ControlNet [@zhang2023adding] and instruction-based image editing
models [@brooks2023instructpix2pix]) in a plug-and-play manner. To the
best of our knowledge, this is the first method that allows inclusive
text-to-image generation over a frozen model and obtains competitive
results throughout.

# Related Work {#s_related}

Text-based image generation has been widely studied with numerous model
architectures and learning
paradigms [@mansimov2015generating; @reed2016generative; @tao2022df; @ramesh2021zero; @gafni2022make; @yu2022scaling; @ding2021cogview; @ding2022cogview2; @chang2023muse; @sohl2015deep; @yang2022diffusion; @croitoru2022diffusion; @dhariwal2021diffusion; @kingma2021variational].
Recently, the overwhelming success of diffusion-based text-to-image
models [@ramesh2022hierarchical; @saharia2022photorealistic; @ramesh2022hierarchical; @nichol2021glide]
has attracted significant attention. A key factor to this success is
their ability to deal with large-scale multimodal
datasets [@schuhmann2022laion; @karras2019style; @chen2015microsoft].
Thus, questions concerning inclusiveness while learning with biased
datasets remain a crucial open
problem [@cho2022dall; @bansal2022well; @agarwal2021evaluating].

::: figure*
![image](figure/fig_framework.pdf){width="1\\linewidth"}
:::

While fairness has been studied extensively in discriminative models
 [@wang2020mitigating; @wang2019balanced; @wang2020towards; @liang2022advances],
research on developing fair generative models is
limited [@zhao2018bias; @jain2020imperfect; @friedrich2023fair; @chuang2023debiasing; @luccioni2023stable].
Most efforts focus on GAN-based
models [@choi2020fair; @ramaswamy2021fair; @jalal2021fairness; @rangwani2022improving; @yu2020inclusive; @kenfack2022repfair; @xu2018fairgan; @tan2020improving; @karakas2022fairstyle; @maluleke2022studying],
restricting their applicability to the emerging diffusion-based
text-to-image models. Recently, there have been some efforts to address
this limitation. For instance, Bansal  [@bansal2022well] proposed to
diversify model outputs by ethical intervention[^2].
Ding  [@ding2021cogview] proposed to directly add attribute words to the
prompt. However, these hard prompt searching methods have limitations
such as being opaque and laborious [@bansal2022well], and not always
generating diverse images
reliably [@hutchinson2022underspecification; @bansal2022well]. In this
work, we incorporate a broad spectrum of attributes beyond social
groups. Moreover, we learn inclusive prompts in the continuous embedding
space, requiring no hard prompt specification.

To learn a fair generative model, Wu  [@wu2022generative] employed
off-the-shelf models, such as CLIP [@radford2021learning] and
pre-trained classifiers, as guidance. Choi  [@choi2020fair] used a
reference dataset to train the model via sample re-weighting. In
contrast, we use reference data in a drastically different way ---
treating the images as proxy signals to guide prompt learning but
without retraining the text-to-image model.

Our method is inspired by Prompt Tuning
(PT) [@lester2021power; @jia2022visual]. Typically, PT methods insert
small learnable modules (, tokens) into the pre-trained models and
fine-tune these modules with downstream tasks while freezing the model
parameters. Recently, PT has been leveraged in personalized
text-to-image
generation [@gal2022image; @ruiz2022dreambooth; @kumari2022multi]. By
providing several reference images with the customized subject, they use
a special token to represent the object by optimizing the token
embedding [@gal2022image; @kumari2022multi] or the diffusion
models [@ruiz2022dreambooth; @kumari2022multi]. This motivates us to
learn the specific token embedding for each attribute category for
inclusiveness. However, we note that the previously mentioned methods
for personalization do not effectively capture the attributes in the
images. Thus, we propose to optimize the directions of the
attribute-specific prompts in the joint vision-language embedding space,
bypassing training text-to-image generative models.

# Inclusive Text-to-Image Generation {#s_approach}

To drive the progress of Inclusive Text-to-Image Generation, we propose
, which creates inclusive prompts that represent various attributes and
their combinations. This is particularly challenging for attributes that
are difficult to describe in language or underrepresented. To address
this, uses readily available reference images as guidance, enabling
unambiguous specification of different attributes.
Figure [\[fig:overview\]](#fig:overview){reference-type="ref"
reference="fig:overview"} illustrates the overall framework. In this
section, we first introduce the framework of in Section , then describe
the details of the learning strategy in Section , and finally discuss
the key properties of in Section .

## Overview {#ss_problemsetup}

Given a pre-trained text-to-image generative model $G$ and a
human-written prompt (, *"a headshot of a person"*) tokenized as
$\bm{T} \in \mathbb{R}^{p \times e}$, where $p$ is the number of tokens
and $e$ is the dimension of the embedding space, we aim to sample equal
(or controllable) numbers of images that can represent any *category*
combination given the *attribute* set $\bm{A}$. Formally,
$$\begin{aligned}
 \label{eq:attribute}
    \bm{A} = \{ \mathcal{A}_m | 1 \leq m \leq M \}; \mathcal{A}_m = \{a^{m}_{k} | 1 \leq k \leq K_m \} 
\end{aligned}$$ contains $M$ different attributes (, perceived gender,
skin tone, .), where $a_{k}^{m}$ records a mutually exclusive category
(, a specific type of skin tone) in attribute $\mathcal{A}_m$ and $K_m$
denotes the number of categories in $\mathcal{A}_m$. Note that $K_m$ may
vary among different attributes.

Inspired by [@lester2021power; @jia2022visual], we propose prompt tuning
for inclusive generation. Specifically, for a given category $a^{m}_{k}$
within attribute $\mathcal{A}_{m}$, we inject $q$ *learnable* tokens
$\bm{S}^{m}_{k} \in \mathbb{R}^{q \times e}$ after the original $\bm{T}$
to construct a new prompt
$\bm{P}_{k}^{m} = [\bm{T}; \bm{S}^{m}_{k}] \in \mathbb{R}^{(p+q) \times e}$.
By querying the model $G$ with $\bm{P}_{k}^{m}$, we can generate images
exhibiting the characteristics of the corresponding category
$a^{m}_{k}$. To differentiate the new tokens $\bm{S}^{m}_k$ from the
original prompt $\bm{T}$, we refer to them as *inclusive tokens*.

When jointly considering $M$ attributes, we aggregate $M$ separate
inclusive tokens
$\bm{S}^{1}_{o_1}, \bm{S}^{2}_{o_2}, \dots, \bm{S}^{M}_{o_M}$ to
represent a specific category combination
$(a^1_{o_1}, a^2_{o_2}, \dots, a^M_{o_M})$, , the concept of ("woman",
"dark skin", $\dots$, "young"). We thus expect to create a unique
$\bm{S}_{o_1 o_2 \dots o_M}$, $$\begin{aligned}
 \label{eq:aggregation}
    \bm{S}_{o_1 o_2 \dots o_M} = f (\bm{S}^{1}_{o_1}, \bm{S}^{2}_{o_2}, \dots, \bm{S}^{M}_{o_M})
\end{aligned}$$ that can be injected after $\bm{T}$ to generate images
for this particular category combination. The aggregation function $f$
in Equation [\[eq:aggregation\]](#eq:aggregation){reference-type="ref"
reference="eq:aggregation"} should be able to take various numbers of
attributes while maintaining the permutation invariant property[^3] with
respect to attributes. Common options include element-wise average, sum,
and max operations. Following [@mikolov2013efficient], we adopt
element-wise sum to preserve the text semantics without losing
information[^4]. Finally, we define the *inclusive prompt set* as
follows: $$\begin{aligned}
 \label{eq:set} 
    \mathcal{P}_\text{total} = \{ &\bm{P}_{o_1 o_2 \dots o_M} = [{\color{blue}{\bm{T}}};\sum_{m=1}^{M} {\color{my_pink}{\bm{S}^{m}_{o_m}}}] \in \mathbb{R}^{(p+q) \times e} ~| \nonumber \\
    & 1 \leq o_1 \leq K_1, \dots, 1 \leq o_M \leq K_M \}.
\end{aligned}$$ By uniformly sampling the prompts from
$\mathcal{P}_\text{total}$ as the conditions to generate images using
the generative model $G$, we achieve inclusiveness across all attributes
(see Figure [\[fig:overview\]](#fig:overview){reference-type="ref"
reference="fig:overview"}). *More generally speaking, the distribution
of the generated data is directly correlated to the distribution of the
prompts, which can be easily controlled.*

In contrast to specifying the category name in discrete language
space [@bansal2022well; @ding2021cogview], we optimize prompts entirely
in the *continuous* embedding space. Additionally, we only update the
attribute-specific embeddings --- the colors and in
Equation [\[eq:set\]](#eq:set){reference-type="ref" reference="eq:set"}
indicate and parameters, respectively. This decoupled optimization
mechanism thus provides the advantage of using the learned inclusive
tokens in a plug-and-play manner across various applications, as will be
demonstrated in Section  and Section . We elaborate on the learning
process in the following section.

## Learning Inclusive Prompts {#ss_objective}

We propose using reference images to guide prompt learning, as they can
provide more expressive signals to describe attributes that may be
challenging to articulate through language. Specifically, we assume the
availability of a reference image set
$\mathcal{D}^{m}_\text{ref} = \{ (\vx_n^m, y_n^m)\}_{n=1}^{N_m}$ for a
target attribute $\mathcal{A}_m$, where $N_m$ is the dataset size and
$y_n^m \in \mathcal{A}_m$ (defined in
Equation [\[eq:attribute\]](#eq:attribute){reference-type="ref"
reference="eq:attribute"}) indicates the category to which $\vx_n$
belongs. When considering multiple attributes, we only need a reference
dataset for each attribute, rather than one large balanced dataset with
all attribute labels. *This property is extremely beneficial, as it is
much easier to obtain a dataset that captures only the distribution of
one attribute (i.e., the marginal distribution) rather than one that
captures the joint distribution of all attributes.*

Given reference image sets for the target attributes, can we learn
prompts that align the attributes in the images? Recently, pre-trained
large-scale multimodal models have demonstrated strong capabilities in
connecting vision and language. One such model is
CLIP [@radford2021learning], which aligns visual concepts with text
embeddings by jointly training a text encoder $E_\text{text}$ and an
image encoder $E_\text{img}$. The output of the pre-trained CLIP text
encoder has also been used as the condition for text-guided image
generation [@rombach2022high; @ramesh2022hierarchical], opening up an
opportunity to align prompts to reference images without the need to
modify the text-to-image models.

One straightforward solution is to maximize the similarity between the
prompt and the reference image embeddings in the CLIP space, as
suggested by [@radford2021learning]. However, we found it deficient for
two reasons. First, this objective forces the prompt to focus on the
overall visual information in the images, rather than the specific
attribute of interest. Second, the generated images from the learned
prompt often exhibit adversarial effects or significant quality
degradation, potentially due to image features distorting the prompt
embedding. To address these, we propose direction alignment and semantic
consistency losses, as described below.

![**Translating visual differences into text embedding differences.**
Given reference images of a multi-category attribute (, skin tone), we
learn the inclusive tokens by direction alignment between and , ensuring
that the visual difference matches the learned language description. In
addition, we propose semantic consistency loss to address language
drift. Images are from FAIR benchmark [@Feng:TRUST:ECCV2022]. Details
are in Section . ](figure/fig_translation.pdf){#fig:approach
width="1\\linewidth"}

Instead of directly maximizing the similarity between the prompts and
the images, we draw inspiration
from [@patashnik2021styleclip; @gal2022stylegan] to induce the direction
between the prompt $\bm{P}^m_i$ and $\bm{P}^m_j$ to be aligned with the
direction between the averaged embeddings of the reference images
corresponding to *a pair of categories* $a_i^m$ and $a_j^m$ in
$\mathcal{A}_m$. This alignment of pairwise categories direction serves
as a proxy task for guiding the prompts to learn the visual difference
among images from category $a_i^m$ and $a_j^m$
(Figure [2](#fig:approach){reference-type="ref"
reference="fig:approach"}).

Specifically, we define the direction alignment loss
$\mathcal{L}_\text{dir}$ to maximize the cosine similarity between the
image direction and the prompt direction as follows: $$\begin{aligned}
 \label{equ:4}
    \mathcal{L}_\text{dir}^{m}(\bm{S}^{m}_i, \bm{S}^{m}_j) = 1 - \bigl\langle \Delta_{\bm{I}}^{m} (i, j), \Delta_{\bm{P}}^{m} (i, j) \bigl\rangle.
\end{aligned}$$ Here, the image direction $\Delta_{\bm{I}}$ is defined
as the difference of the averaged image embeddings between two
categories of the attribute $\mathcal{A}_m$. Let
$\mathfrak{X}_{k}^{m} = \frac{1}{|\mathcal{B}_k|}\sum_{y_n^m = a_k^m} E_\text{img} (\vx_n^m)$
be the averaged image embedding for category $a_k^m$; $|\mathcal{B}_k|$
is the number of images from category $a_k^m$ in each mini-batch. We
denote the image direction as follows: $$\begin{aligned}
    \Delta_{\bm{I}}^{m} (i, j) = \mathfrak{X}_{i}^{m} - \mathfrak{X}_{j}^{m}.
\end{aligned}$$ Similarly, the prompt direction $\Delta_{\bm{P}}$ is
defined as the difference of the averaged prompt embeddings between two
categories. Let
$\mathfrak{P}_{k}^{m} = \frac{1}{|\mathcal{P}_k^m|} \sum_{\bm{P} \in \mathcal{P}^m_k} E_\text{text}(\bm{P})$
be the averaged prompt embedding for attribute $a_k^m$. Specifically,
$\mathcal{P}^{m}_{k} = \{ \bm{P} \in \mathcal{P}_\text{total} ~|~ o_m = k \}$
is a collection of prompts containing all the category combinations for
other attributes given the category $a_k^m$ for attribute
$\mathcal{A}_m$ (cf. Equation [\[eq:set\]](#eq:set){reference-type="ref"
reference="eq:set"}). Finally, we denote the prompt direction as
follows: $$\begin{aligned}
    \Delta^{m}_{\bm{P}} (i, j) = \mathfrak{P}_{i}^{m} -  \mathfrak{P}_{j}^{m}.
\end{aligned}$$ By inducing the direction alignment, we aim to
facilitate the prompt learning of more meaningful and nuanced
differences between images from different categories.

We observe that direction alignment loss alone may result in language
drift [@lu2020countering; @lee2019countering; @ruiz2022dreambooth] ---
the prompts slowly lose syntactic and semantic properties of language as
they only focus on solving the alignment task. To resolve this issue, we
design a semantic consistency objective to regularize the training by
maximizing the cosine similarity between the learning prompts and the
original input prompt (see
Figure [2](#fig:approach){reference-type="ref"
reference="fig:approach"}): $$\begin{aligned}
 \label{equ:7}
    \mathcal{L}_\text{sem}^{m}(\bm{S}^{m}_i, \bm{S}^{m}_j) = \text{max} \Bigl(0, \lambda - \bigl \langle E_\text{text}(\bm{P}), E_\text{text}(\bm{T}) \bigl\rangle \Bigl)
\end{aligned}$$ where $\bm{P} \in \mathcal{P}^m_i \cup \mathcal{P}^m_j$
and $\lambda$ is a hyperparameter (see an analysis in Section ). This
loss is crucial for generating high-quality images that remain faithful
to the input prompt.

Building upon $\mathcal{L}_\text{dir}^m$ and $\mathcal{L}_\text{sem}^m$,
our total training loss for learning the inclusive tokens of a pair of
categories in attribute $\mathcal{A}_m$ is written as follows:
$$\begin{aligned}
    \mathcal{L}_\text{pair}^{m} (\bm{S}_i^m, \bm{S}_j^m) = \mathcal{L}^{m}_\text{dir} (\bm{S}_i^m, \bm{S}_j^m) + \mathcal{L}^{m}_\text{sem} (\bm{S}_i^m, \bm{S}_j^m). 
\end{aligned}$$ At each iteration, we update the embeddings of inclusive
tokens of all the categories from *only one attribute* but freeze the
parameters of inclusive tokens for all other attributes. The final
objective during the whole learning process is: $$\begin{aligned}
 %\label{eq:multi_loss}
    \mathcal{L}_\text{total} = {\color{blue}{\sum_{m=1}^{M}}} {\color{my_pink}{\sum_{1 \leq i < j \leq K_m}}}  \mathcal{L}_\text{pair}^{m} (\bm{S}_i^m, \bm{S}_j^m),
\end{aligned}$$ where the enumerates all pairwise categories for one
attribute $\mathcal{A}_m$ at each iteration, while the outer summation
alters the attribute across the iteration.

## Key Properties of  {#ss_others}

Unlike personalization methods that train the embeddings for a specific
model (because they use diffusion
losses [@gal2022image; @kumari2022multi; @ruiz2022dreambooth]), *the
tokens learned by are transferable between different models.* We
highlight two use cases for these tokens. (1) *In-domain generation.* We
use the user-specified prompt $\bm{T}$ to learn the inclusive tokens and
then apply them back to $\bm{T}$ to generate inclusive images. (2)
*Train-once-for-all.* As shown in
Equation [\[eq:set\]](#eq:set){reference-type="ref" reference="eq:set"},
the newly introduced inclusive tokens do not change the original prompt
$\bm{T}$, which implies that the learned tokens can be compatible with a
different human-written prompt. For human face images, an example
$\bm{T}$ for training can be any neutral prompt, , *"a headshot of a
person"*. After training, inclusive tokens can be used to handle
out-of-domain prompts (, *"a photo of a doctor"*) or facilitate
different models [@zhang2023adding; @brooks2023instructpix2pix] in a
plug-and-play manner, justifying the generalizability of our approach.

uses averaged image features to guide prompt learning, indicating that
(1) only a few dozen images per category are sufficient, and (2) a
balanced distribution across categories within an attribute is *not*
required. keeps the text-to-image model intact and only updates the
inclusive tokens, allowing it to circumvent the costly back-propagation
step in the diffusion model. Training with a single attribute takes
approximately 5 minutes (1 A4500 GPU). In practice, we set the
length[^5] ($q$ in Equation [\[eq:set\]](#eq:set){reference-type="ref"
reference="eq:set"}) of inclusive tokens to $3$ (which is less than
10KB) for all attribute categories of interest in our study. Hence, when
scaling up to scenarios with multiple attributes, always has low memory
requirements for both training and storing inclusive tokens.

Our direction alignment loss may be reminiscent of the directional CLIP
loss employed in image editing
methods [@gal2022stylegan; @kim2022diffusionclip]. However, they are
fundamentally different. First, our is designed to promote the
inclusiveness, while image editing methods focus on single image
manipulation. Second, image editing methods modify the source image
according to the change in texts (from source to target), whereas learns
prompts by leveraging changes in images from one category to another.
This key difference suggests a significant distinction: the two methods
are learning the task from completely different directions.

# Experiments {#s_exp}

We validate for inclusive text-to-image generation on various attributes
and scenarios. We begin by introducing the experimental setup in
Section , then present the main results in Section , and finally, show
detailed ablation studies and applications in Section . Please see
Appendix for additional details, results, and analyses.

## Setup {#exp_setup}

::: table*
::: center
  **Method**                              **(a) Single Attribute**                                                                                                                                                                                                                                                                                                **(b) Multiple Attributes**                                                                                                                   
  ---------------------------- ----------------------------------------------- ------------------------------------------------ ---------------------------------------------------- --------------------------------------------------- --------------------------------------------------- ------------------------------------------------ ------------------------------------------------------------------- --------------------------------------------------------------------------------------------- ----------------------------------------------------------------------------------------------------------------
  2-10                          $\mathbb{D}_\text{KL}^\text{male} \downarrow$   $\mathbb{D}_\text{KL}^\text{young} \downarrow$   $\mathbb{D}_\text{KL}^\text{pale skin} \downarrow$   $\mathbb{D}_\text{KL}^\text{eyeglass} \downarrow$   $\mathbb{D}_\text{KL}^\text{mustache} \downarrow$   $\mathbb{D}_\text{KL}^\text{smile} \downarrow$   $\mathbb{D}_\text{KL}^{\text{male}\times\text{young}} \downarrow$   $\mathbb{D}_\text{KL}^{\text{male} \times  \text{young} \times \text{eyeglass}} \downarrow$   $\mathbb{D}_\text{KL}^{\text{male} \times \text{young} \times \text{eyeglass} \times \text{smile}} \downarrow$
  1-10 SD [@rombach2022high]                        0.343                                           0.578                                              0.308                                                0.375                                               0.111                                             0.134                                                      0.882                                                                            1.187                                                                                                  1.406
  EI [@bansal2022well]                              0.143                                           0.423                                              0.644                                                0.531                                               0.693                                             0.189                                                      0.361                                                                            1.054                                                                                                  1.311
  HPS [@ding2021cogview]              1 $\times \text{10}^{-\text{5}}$                              0.027                                2.8 $\times \text{10}^{-\text{3}}$                                 0.371                                               0.241                               4.4 $\times \text{10}^{-\text{3}}$                        3.5 $\times \text{10}^{-\text{3}}$                                                              0.399                                                                                                  0.476
  PD [@chuang2023debiasing]                         0.322                                           0.131                                              0.165                                                0.272                                               0.063                                             0.146                                                       --                                                                               --                                                                                                      --
  CD [@kumari2022multi]                             0.309                                           0.284                                              0.074                                                0.301                                               0.246                                             0.469                                                       --                                                                               --                                                                                                      --
  1-10                              **2 $\times \text{10}^{-\text{6}}$**             **2 $\times \text{10}^{-\text{4}}$**                              **0**                                **2 $\times \text{10}^{-\text{4}}$**               **4.5 $\times \text{10}^{-\text{4}}$**             **2.5 $\times \text{10}^{-\text{3}}$**                    **1.3 $\times \text{10}^{-\text{4}}$**                                                          **0.061**                                                                                              **0.094**
:::
:::

::: figure*
![image](figure/fig_combination.pdf){width="1\\linewidth"}
:::

![**Examples of reference images.** CelebA [@liu2015faceattributes] and
FairFace [@karkkainen2019fairface] are real-face datasets with different
resolutions and focuses. FAIR benchmark [@Feng:TRUST:ECCV2022] is a
synthetic dataset used for skin tone estimation. Landscape
(LHQ) [@skorokhodov2021aligning] contains images from natural scenes.
can leverage various image sources to benefit inclusive text-to-image
generation for various attributes.](figure/fig_sample.pdf){#fig:sample
width="1\\linewidth"}

We construct reference image sets and investigate a variety of
attributes based on the following datasets. **(1)
CelebA** [@liu2015faceattributes] is a face attributes dataset and each
image with $40$ binary attribute annotations. We experiment with these
binary attributes and their combinations. **(2) FAIR benchmark
(FAIR)** [@Feng:TRUST:ECCV2022] is a recently proposed synthetic face
dataset used for skin tone estimation. Following [@Feng:TRUST:ECCV2022],
we use the ground-truth albedos to classify each facial crop into one of
six skin tone levels [@fitzpatrick1988validity] and use FAIR for
inclusiveness on skin tone type. **(3)
FairFace** [@karkkainen2019fairface][^6] contains face images with
annotations for $2$ perceived gender and $9$ perceived age categories.
**(4) Landscapes HQ (LHQ)** [@skorokhodov2021aligning] provides
unlabeled natural scene images. With the annotation tool from
[@wang2022exploring], each image can be labeled with $6$ quality (,
colorfulness, brightness) and $6$ abstraction (, scary, aesthetic)
attributes. Figure [3](#fig:sample){reference-type="ref"
reference="fig:sample"} shows example images.

We only require that a reference image set captures a marginal
distribution for each attribute (cf. Section ). Note that, while images
from CelebA and FairFace are annotated with multiple attributes, we use
only the attribute label for each target category but not others. We
randomly select $25$ reference images per category as our default
setting (and ablate it in Section ). For attribute settings, we consider
*single binary attribute*, *multi-category attributes*, and *multiple
attributes* in the domains of human faces and scenes. We study both
in-domain and train-once-for-all generations (cf. Section ) and further
provide qualitative and quantitative analyses for each setup.

We use two metrics to quantify distribution diversity and image quality.
(1) *Distribution Discrepancy* ($\mathbb{D}_\text{KL}$).
Following [@cho2022dall; @chuang2023debiasing], we use the CLIP model to
predict the attributes in the images. For attributes that CLIP might be
erroneous, we leverage pre-trained classifiers [@karkkainen2019fairface]
combined with human evaluations. Specifically, for skin tone, which is
extreme difficult to obtain an accurate
scale [@Shades; @Google; @howard2021reliability], we adopt the most
commonly used Fitzpatrick skin type [@chardon1991skin] combined with
off-the-shelf models [@Feng:TRUST:ECCV2022] for evaluation. (2) *FID.*
We report the FID score [@heusel2017gans; @parmar2022aliased]
(FFHQ [@karras2019style]) to measure the image quality. Please see
Appendix [\[app_s_additional_ablation\]](#app_s_additional_ablation){reference-type="ref"
reference="app_s_additional_ablation"} for more details.

We compare to the following methods. (1) *Stable Diffusion*
(SD) [@rombach2022high] without any modification. (2) *Ethical
Intervention* (EI) [@bansal2022well] that edits the prompt by adding
attribute-related interventions. (3) *Hard Prompt Searching*
(HPS) [@ding2021cogview] that directly expresses the desired attribute
category in the prompt. (4) *Prompts Debiasing*
(PD) [@chuang2023debiasing] that calibrates the bias in the text
embedding by using the attribute category names. (5) *Custom Diffusion*
(CD) [@kumari2022multi] that fine-tunes the text-to-image model with
reference images based on Textual
Inversion [@gal2022image; @ruiz2022dreambooth].

We use Stable Diffusion [@rombach2022high] (sd-v1-4) as the base model
for all methods and show compatibility with
ControlNet [@zhang2023adding] and
InstructPix2Pix [@brooks2023instructpix2pix]. is model agnostic as long
as they take token embeddings as the inputs. We set $\lambda=0.8$ in
$\mathcal{L}_\text{sem}$ across all experiments and show that $\lambda$
can be robustly selected according to the prior knowledge (see
Section ). All the inclusive tokens are initiated as zero vectors[^7].
We set the length of the inclusive tokens to $3$ in all experiments.
*There is no additional hyper-parameter in our framework.* The total
number of the parameters for the inclusive tokens that need to be
optimized is $\sum^{M}_{m=1} K_m \times 3 \times 768$, where $M$ is the
number of attributes, $K_m$ is the category number for attribute $m$,
and $768$ is the dimension of the embedding ($e$ in
Equation [\[eq:set\]](#eq:set){reference-type="ref"
reference="eq:set"}). We train the models with $30$ epochs on a batch
size of 16 and a learning rate of $0.01$. During training, we leverage
image augmentations used in the CLIP image encoder.

![**Multi-category distribution** with "*a headshot of a person*". For a
reliable evaluation, the results of (a) are evaluated using classifiers
in [@karkkainen2019fairface], and (b) are evaluated using existing
models [@chardon1991skin; @Feng:TRUST:ECCV2022]. The generated images
from are more uniformly distributed across different sub-groups than the
baseline Stable Diffusion. See
Figure [5](#fig:multi_category_qual){reference-type="ref"
reference="fig:multi_category_qual"} for qualitative
results.](figure/figure_multi_category_hist.pdf){#fig:multi_category_hist
width="1\\linewidth"}

## Main Results {#ss_results}

To demonstrate the capability of to sample images with a variety of face
attributes, we construct $40$ distinct reference image sets based on
attributes from CelebA [@liu2015faceattributes]. Each represents a
specific binary attribute and contains an equal number of images
($50\%$) for the positive and negative categories[^8].
Table [\[tab:celeba\]](#tab:celeba){reference-type="ref"
reference="tab:celeba"}(a) shows a comparison to state-of-the-art
methods. We evaluate $5$ text prompts --- "*a headshot of a {person,
professor, doctor, worker, firefighter}*" --- and sample $200$ images
per prompt for each attribute, resulting in $40$K generated images. We
highlight the averaged results across $5$ prompts of $6$ attributes. We
provide complete results in
Appendix [\[app_ss_single\]](#app_ss_single){reference-type="ref"
reference="app_ss_single"}. achieves near-perfect performance on
balancing each binary attribute, justifying our motivation: using
separate inclusive tokens is beneficial in generating images that are
uniformly distributed across attribute categories.

Given multiple reference image sets (each captures the marginal
distribution for an attribute), can generate diverse images across any
category combination of the attributes? We provide an affirmative answer
and present results in
Table [\[tab:celeba\]](#tab:celeba){reference-type="ref"
reference="tab:celeba"}(b) and
Figure [\[fig:2222\]](#fig:2222){reference-type="ref"
reference="fig:2222"}. As we observe, produces diverse and high-quality
images with significantly lower distribution discrepancies compared to
baseline methods. We attribute this to the aggregation operation of
inclusive tokens (Equation [\[eq:set\]](#eq:set){reference-type="ref"
reference="eq:set"}), allowing to disentangle the learning of different
inclusive tokens with images in marginal distributions.

![**Results of on multi-category attributes** for Gender$\times$Age
(Figure [4](#fig:multi_category_hist){reference-type="ref"
reference="fig:multi_category_hist"}(a)) and Gender$\times$Skin Tone
(Figure [4](#fig:multi_category_hist){reference-type="ref"
reference="fig:multi_category_hist"}(b)). Examples are randomly picked
with "*a headshot of a
person*".](figure/fig_multi_category_qual.pdf){#fig:multi_category_qual
width="1\\linewidth"}

![**with perception attributes on scene images.** The tokens of
"colorfulness" are trained with "*a photo of a natural scene*" and
applied to "*a castle on the cliff*" in this example
(*train-once-for-all* in Section ). (right) enables the baseline Stable
Diffusion (left) to generate images with different levels of
colorfulness. Same seed for each row. Better viewed in color. See
Appendix [\[app_ss_other_domains\]](#app_ss_other_domains){reference-type="ref"
reference="app_ss_other_domains"} for results of other attributes, ,
scary, brightness.](figure/fig_scene.pdf){#fig:scene
width="1\\linewidth"}

We further investigate multi-category attributes including perceived age
and skin tone. Specifically, we consider two challenging settings: (1)
Perceived Gender $\times$ Age
(Figure [4](#fig:multi_category_hist){reference-type="ref"
reference="fig:multi_category_hist"}(a)), and (2) Perceived Gender
$\times$ Skin Tone
(Figure [4](#fig:multi_category_hist){reference-type="ref"
reference="fig:multi_category_hist"}(b)). achieves inclusiveness across
all setups, especially on extremely underrepresented categories for age
($<$ 10 and $>50$ years old in
Figure [4](#fig:multi_category_hist){reference-type="ref"
reference="fig:multi_category_hist"}(a)). More surprisingly
(Figure [4](#fig:multi_category_hist){reference-type="ref"
reference="fig:multi_category_hist"}(b)), can leverage synthetic images
(from FAIR) and jointly learn from different data sources (CelebA for
gender and FAIR for skin tone), demonstrating great potential for
bootstrapping inclusive data generation with graphics engines.

Besides human faces, we apply to another domain: scene images. We claim
that the inclusive text-to-image generation accounts for attributes from
not only humans but also scenes, objects, or even environmental factors.
Specifically, we use images from LHQ [@skorokhodov2021aligning] as
guidance to learn inclusive tokens and generate images with diverse
subjective perception attributes. As illustrated in
Figure [6](#fig:scene){reference-type="ref" reference="fig:scene"}, can
enrich the generated images to multiple levels of colorfulness[^9],
justifying the generalizability of our method to the attributes in
different domains.

![**Ablation on the quantity of reference images.** More reference
images ($>10$) help possibly due to more diversity and less noise. is
robust in the low data regime
(Section ).](figure/fig_ablation_quantity.pdf){#fig:quan_refer_img
width="1.0\\linewidth"}

::: {#tab:fid}
  **Method**                                     **Source**               **$\mathcal{L}_\text{sem}$**    **FID**$\downarrow$
  --------------------------------- ------------------------------------ ------------------------------ ---------------------
  1-4 Baseline [@rombach2022high]                    --                                --                               67.40
  1-4                                 CelebA [@liu2015faceattributes]                                               **60.38**
                                                                                                                        77.78
  2-4                                FairFace [@karkkainen2019fairface]                                             **55.10**
                                                                                                                        64.11
  2-4                                   FAIR [@Feng:TRUST:ECCV2022]                                                 **51.83**
                                                                                                                        62.69

  : **Ablation on reference image sources and
  $\mathcal{L}_\text{sem}$.** produces lower FID than the baseline
  Stable Diffusion. Semantic consistency loss $\mathcal{L}_\text{sem}$
  plays a key role in quality control.
:::

## Ablations and Applications {#ss_ablation}

Figure [7](#fig:quan_refer_img){reference-type="ref"
reference="fig:quan_refer_img"} illustrates the impact of the *quantity*
of reference images per attribute category, telling that can produce
high-quality images using very few reference data without sacrificing
inclusiveness (KL). In addition, as indicated in
Table [1](#tab:fid){reference-type="ref" reference="tab:fid"},
consistently generates realistic images regardless of reference sources
(see examples in Figure [\[fig:2222\]](#fig:2222){reference-type="ref"
reference="fig:2222"} and
Figure [5](#fig:multi_category_qual){reference-type="ref"
reference="fig:multi_category_qual"}). More interestingly, we found that
using synthetic images (, FAIR [@Feng:TRUST:ECCV2022]) is slightly
better than real data [@liu2015faceattributes; @karkkainen2019fairface].
We hypothesize that the background noise in real images degrades the
quality.

Again in Table [1](#tab:fid){reference-type="ref" reference="tab:fid"},
we compare with and without $\mathcal{L}_\text{sem}$. With the help of
the semantic constraint (Figure [2](#fig:approach){reference-type="ref"
reference="fig:approach"}), we regularize the learned embeddings not too
far from the original prompt. We show evidence to verify this insight:
the averaged CLIP similarity scores of text features between the hard
prompts of $40$ attributes in CelebA and the original prompt is $0.8$
(the $\lambda$ we used), suggesting that the hyper-parameter can be
robustly chosen based on prior linguistic knowledge.

![**Train-once-for-all generalization.** Inclusive tokens of trained
with a neutral prompt ("*a headshot of a person*") can be applied to
out-of-domain prompts in these two examples to alleviate stereotypes.
See
Appendix [\[app_ss_train_once\]](#app_ss_train_once){reference-type="ref"
reference="app_ss_train_once"} for more
results.](figure/fig_trainone_useevery.pdf){#fig:prepend
width="1.0\\linewidth"}

<figure id="fig:controlNet">
<div class="center">
<embed src="figure/fig_controlnet.pdf" />
</div>
<figcaption><strong>Compatibility with models using additional
conditions</strong>, , human pose (left). promotes inclusiveness of
ControlNet <span class="citation" data-cites="zhang2023adding"></span>
by using the inclusive tokens of six skin tone types (right). The tokens
are trained with “<em>a headshot of a person</em>” guided by images from
FAIR dataset <span class="citation"
data-cites="Feng:TRUST:ECCV2022"></span>, and applied here in a
<em>train-once-for-all</em> manner (Section ). See Appendix <a
href="#app_ss_compatible" data-reference-type="ref"
data-reference="app_ss_compatible">[app_ss_compatible]</a> for
additional results on versatile conditions, , depth,
segmentation.</figcaption>
</figure>

As shown in Figure [6](#fig:scene){reference-type="ref"
reference="fig:scene"}, inclusive tokens can be applied to
user-specified prompts in a plug-and-play manner (Section ). In
Figure [8](#fig:prepend){reference-type="ref" reference="fig:prepend"},
we provide more examples of professional prompts to demonstrate the
ability of train-once-for-all generation.

 [@zhang2023adding]. achieves inclusiveness by learning
attribute-specific prompts without modifying the original text-to-image
model, potentially benefiting various downstream vision-language tasks.
In Figure [9](#fig:controlNet){reference-type="ref"
reference="fig:controlNet"}, we demonstrate its compatibility with
ControlNet [@zhang2023adding], a state-of-the-art model capable of
conditioning on a variety of inputs beyond text. Interestingly, we
observe an intriguing feature where the newly introduced tokens may
implicitly entangle other biases or contrasts inherent in the reference
image sets, such as clothing style. Nevertheless, we emphasize that
disentanglement of attributes is not the primary concern of this study.
achieves competitive results in distributional control for the
*intended* attributes (, skin tone in
Figure [9](#fig:controlNet){reference-type="ref"
reference="fig:controlNet"}) --- aggregating tokens learned from
marginal distributions implicitly disentangles the *known* attributes of
interest.

 [@brooks2023instructpix2pix]. Note that, achieving fully unsupervised
disentanglement is a challenging task [@locatello2019challenging].
Previous attempts in image generation often resort to additional
supervision, either through the use of reference data [@choi2020fair],
classifiers learned from a joint distribution [@tan2020improving], or
even more robust controls such as instruction-based image
editing [@brooks2023instructpix2pix]. Here, we show that can potentially
disentangle the target attribute by incorporating
InstructPix2Pix [@brooks2023instructpix2pix] --- to improve the
inclusiveness of IP2P on the target attribute, while ensuring minimal
changes to other features such as clothing and background. Results are
shown in Figure [10](#fig:ip2p){reference-type="ref"
reference="fig:ip2p"}, telling that can be an effective method to
condition diffusion on contrastive image sets, , images taken by
different cameras, art by unknown artists, and maybe even different
identities of people.

![**Compatibility with instruction-based image editing methods.** Given
an image and a written instruction (top-left), InstructPix2Pix
(IP2P) [@brooks2023instructpix2pix] follows the instruction to edit the
image (bottom-left). (right) enables inclusive instruction-based image
editing. Similar to Figure [9](#fig:controlNet){reference-type="ref"
reference="fig:controlNet"}, the inclusive tokens used in this example
are trained in a train-once-for-all manner.](figure/IP2P.pdf){#fig:ip2p
width="1\\linewidth"}

# Conclusion and Discussion {#s_disc}

We present a new method for inclusive text-to-image generation. Our main
contribution lies in a new direction: *leveraging readily available
reference images to improve the inclusiveness of text-to-image
generation.* This problem is timely and
challenging [@bianchi2023easily; @bansal2022well; @chuang2023debiasing; @friedrich2023fair; @cho2022dall].
Our key insight is learning separate token embeddings to represent
different attributes of interest via image guidance. The proposed method
is simple, compact, generalizable, and effective on various
applications. Specifically, has several advantages: (1) scalable to
multiple attributes and different domains using relatively small numbers
of images; (2) can be used in a plug-and-play manner to
out-of-distribution, relatively complex prompts; (3) efficient in both
training and inference; (4) compatible with the text-to-image generative
models that support additional conditions or instructions. We conduct
extensive experiments to verify the effectiveness of the proposed method
on multiple domains, offering insights into various modeling choices and
mechanisms of . We incorporate a broad spectrum of attributes in both
human faces and scenes. We hope that our results and insights can
encourage more future works on exploring inclusive data generation.

can handle a wide range of general attributes, such as perceived gender
and skin tone, and excels in cases where "Hard Prompt" struggles.
However, there remain several limitations. First, does not always
provide optimal results for very subtle facial attributes
(Appendix [\[app_ss_single\]](#app_ss_single){reference-type="ref"
reference="app_ss_single"}) or for the combinations of highly entangled
attributes
(Appendix [\[app_ss_multiple\]](#app_ss_multiple){reference-type="ref"
reference="app_ss_multiple"}). Second, still requires dozens of
reference images for each category as guidance. It is possible that the
reference images may introduce biases or inaccuracies. One mitigation
strategy is to integrate with models that offer robust
controls [@brooks2023instructpix2pix], such as the one highlighted in
Figure [10](#fig:ip2p){reference-type="ref" reference="fig:ip2p"}.

[^1]: Few works [@cho2022dall; @bansal2022well] have studied fairness
    issues in text-to-image generation but mainly focused on social
    biases (, perceived gender, ethnicity). This paper incorporates a
    broader spectrum of attributes.

[^2]: , appending "irrespective of their gender" to the end of a neutral
    prompt "a photo of a lawyer" for generating diverse pictures w.r.t.
    genders.

[^3]: That is, the output of $f$ should be the same even if we permute
    the indices $m$ of the attributes in $\bm{A}$ (cf.
    Equation [\[eq:attribute\]](#eq:attribute){reference-type="ref"
    reference="eq:attribute"}).

[^4]: Please see
    Appendix [\[app_ss_aggregation\]](#app_ss_aggregation){reference-type="ref"
    reference="app_ss_aggregation"} for more analysis and other options
    for aggregating multiple tokens, , concatenation.

[^5]: The token length used here is generalizable across the attributes
    we studied in this paper. See
    Appendix [\[app_ss_length\]](#app_ss_length){reference-type="ref"
    reference="app_ss_length"} for a detailed ablation study.

[^6]: We note that, while the FairFace dataset contains race categories,
    we focus instead on skin tone in this study. This is because skin
    tone is more readily inferable from pixels, whereas racial
    identities are better understood as social concepts that are neither
    immutable nor biological in
    nature [@browne2015dark; @crawford2021atlas; @ray2022critical; @andrews2023ethical];
    furthermore, phenotypic variation of skin tone within racial
    identification groups is well documented [@monk2021unceasing].

[^7]: We investigated other options such as random initialization but
    did not see notable differences in both generation quality and
    training speed.

[^8]: We found that different ratios do not lead to notable differences.
    We provide an analysis of learning with imbalanced data in
    Appendix [\[app_ss_imbalanced\]](#app_ss_imbalanced){reference-type="ref"
    reference="app_ss_imbalanced"}.

[^9]: Note that the subjective attributes we explore here are different
    from artistic styles (, painting, cartoon) in image-to-image
    translation (, [@gal2022stylegan]). Understanding the attributes
    related to *quality* and *look* of images may be intuitive for
    humans but remain non-trivial for generative models.
